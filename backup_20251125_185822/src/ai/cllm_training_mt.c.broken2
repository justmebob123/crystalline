/**
 * Multi-Threaded Training with Gradient Accumulation - FIXED VERSION
 * 
 * Proper implementation:
 * 1. Each thread gets its own gradient buffers
 * 2. Threads process batches independently
 * 3. Gradients are accumulated after all threads finish
 * 4. Single optimizer step with accumulated gradients
 */

#include "cllm_training.h"
#include <pthread.h>
#include <stdlib.h>
#include <string.h>
#include <stdio.h>

// Thread-local gradient storage
typedef struct {
    // Embedding gradients
    float* embedding_grads;
    
    // Per-layer gradients
    struct {
        float* query_lattice;
        float* key_lattice;
        float* value_lattice;
    }* attention_grads;
    
    struct {
        float* w1_lattice;
        float* w2_lattice;
        float* bias1;
        float* bias2;
    }* ff_grads;
    
    struct {
        float* gamma;
        float* beta;
    }* ln_grads;
    
    // Thread state
    int thread_id;
    float thread_loss;
    int batches_processed;
} ThreadGradients;

// Thread work context
typedef struct {
    CLLMTraining* training;
    ThreadGradients* thread_grads;
    int thread_id;
    int num_threads;
    
    // Batch assignment
    int start_batch;
    int end_batch;
    
    // Synchronization
    pthread_mutex_t* training_mutex;  // Protects forward/backward calls
    
    // Results
    float total_loss;
    int batches_completed;
} ThreadContext;

// Multi-threaded training state
typedef struct {
    CLLMTraining* training;
    ThreadContext* thread_contexts;
    int num_threads;
    
    // Synchronization
    pthread_mutex_t mutex;
    
    // Batch management
    int total_batches;
    
    // Accumulated results
    float total_loss;
    int total_batches_processed;
} MTTrainingState;

/**
 * Allocate thread-local gradient buffers
 */
static ThreadGradients* allocate_thread_gradients(CLLMModel* model) {
    ThreadGradients* tg = (ThreadGradients*)calloc(1, sizeof(ThreadGradients));
    if (!tg) return NULL;
    
    uint32_t vocab_size = model->vocab_size;
    uint32_t embed_dim = model->embedding_dim;
    uint32_t num_layers = model->num_layers;
    
    // Allocate embedding gradients
    tg->embedding_grads = (float*)calloc(vocab_size * embed_dim, sizeof(float));
    if (!tg->embedding_grads) {
        free(tg);
        return NULL;
    }
    
    // Allocate attention gradients
    tg->attention_grads = calloc(num_layers, sizeof(*tg->attention_grads));
    if (!tg->attention_grads) {
        free(tg->embedding_grads);
        free(tg);
        return NULL;
    }
    
    for (uint32_t i = 0; i < num_layers; i++) {
        uint64_t attn_size = embed_dim * embed_dim;
        tg->attention_grads[i].query_lattice = (float*)calloc(attn_size, sizeof(float));
        tg->attention_grads[i].key_lattice = (float*)calloc(attn_size, sizeof(float));
        tg->attention_grads[i].value_lattice = (float*)calloc(attn_size, sizeof(float));
        
        if (!tg->attention_grads[i].query_lattice || 
            !tg->attention_grads[i].key_lattice || 
            !tg->attention_grads[i].value_lattice) {
            // Cleanup on failure
            for (uint32_t j = 0; j <= i; j++) {
                free(tg->attention_grads[j].query_lattice);
                free(tg->attention_grads[j].key_lattice);
                free(tg->attention_grads[j].value_lattice);
            }
            free(tg->attention_grads);
            free(tg->embedding_grads);
            free(tg);
            return NULL;
        }
    }
    
    // Allocate feedforward gradients
    tg->ff_grads = calloc(num_layers, sizeof(*tg->ff_grads));
    if (!tg->ff_grads) {
        for (uint32_t i = 0; i < num_layers; i++) {
            free(tg->attention_grads[i].query_lattice);
            free(tg->attention_grads[i].key_lattice);
            free(tg->attention_grads[i].value_lattice);
        }
        free(tg->attention_grads);
        free(tg->embedding_grads);
        free(tg);
        return NULL;
    }
    
    for (uint32_t i = 0; i < num_layers; i++) {
        uint64_t ff_size = embed_dim * embed_dim;
        tg->ff_grads[i].w1_lattice = (float*)calloc(ff_size, sizeof(float));
        tg->ff_grads[i].w2_lattice = (float*)calloc(ff_size, sizeof(float));
        tg->ff_grads[i].bias1 = (float*)calloc(embed_dim, sizeof(float));
        tg->ff_grads[i].bias2 = (float*)calloc(embed_dim, sizeof(float));
    }
    
    // Allocate layer norm gradients
    tg->ln_grads = calloc(num_layers, sizeof(*tg->ln_grads));
    if (!tg->ln_grads) {
        for (uint32_t i = 0; i < num_layers; i++) {
            free(tg->ff_grads[i].w1_lattice);
            free(tg->ff_grads[i].w2_lattice);
            free(tg->ff_grads[i].bias1);
            free(tg->ff_grads[i].bias2);
            free(tg->attention_grads[i].query_lattice);
            free(tg->attention_grads[i].key_lattice);
            free(tg->attention_grads[i].value_lattice);
        }
        free(tg->ff_grads);
        free(tg->attention_grads);
        free(tg->embedding_grads);
        free(tg);
        return NULL;
    }
    
    for (uint32_t i = 0; i < num_layers; i++) {
        tg->ln_grads[i].gamma = (float*)calloc(embed_dim, sizeof(float));
        tg->ln_grads[i].beta = (float*)calloc(embed_dim, sizeof(float));
    }
    
    return tg;
}

/**
 * Free thread-local gradient buffers
 */
static void free_thread_gradients(ThreadGradients* tg, uint32_t num_layers) {
    if (!tg) return;
    
    free(tg->embedding_grads);
    
    if (tg->attention_grads) {
        for (uint32_t i = 0; i < num_layers; i++) {
            free(tg->attention_grads[i].query_lattice);
            free(tg->attention_grads[i].key_lattice);
            free(tg->attention_grads[i].value_lattice);
        }
        free(tg->attention_grads);
    }
    
    if (tg->ff_grads) {
        for (uint32_t i = 0; i < num_layers; i++) {
            free(tg->ff_grads[i].w1_lattice);
            free(tg->ff_grads[i].w2_lattice);
            free(tg->ff_grads[i].bias1);
            free(tg->ff_grads[i].bias2);
        }
        free(tg->ff_grads);
    }
    
    if (tg->ln_grads) {
        for (uint32_t i = 0; i < num_layers; i++) {
            free(tg->ln_grads[i].gamma);
            free(tg->ln_grads[i].beta);
        }
        free(tg->ln_grads);
    }
    
    free(tg);
}

/**
 * Saved gradient pointers for each thread
 */
typedef struct {
    float* saved_gradients;
    void* saved_attention_grads;
    void* saved_ff_grads;
    void* saved_ln_grads;
} SavedGradients;

/**
 * Swap gradient buffers - temporarily replace training gradients with thread-local ones
 */
static void swap_gradient_buffers(CLLMTraining* training, ThreadGradients* tg, SavedGradients* saved, int restore) {
    if (!restore) {
        // Save original pointers
        saved->saved_gradients = training->gradients;
        saved->saved_attention_grads = training->attention_grads;
        saved->saved_ff_grads = training->ff_grads;
        saved->saved_ln_grads = training->ln_grads;
        
        // Replace with thread-local buffers
        training->gradients = tg->embedding_grads;
        training->attention_grads = tg->attention_grads;
        training->ff_grads = tg->ff_grads;
        training->ln_grads = tg->ln_grads;
    } else {
        // Restore original pointers
        training->gradients = saved->saved_gradients;
        training->attention_grads = saved->saved_attention_grads;
        training->ff_grads = saved->saved_ff_grads;
        training->ln_grads = saved->saved_ln_grads;
    }
}

/**
 * Worker thread function
 */
static void* worker_thread(void* arg) {
    ThreadContext* ctx = (ThreadContext*)arg;
    CLLMTraining* training = ctx->training;
    ThreadGradients* tg = ctx->thread_grads;
    
    printf("DEBUG: Thread %d starting, batches %d to %d\n", ctx->thread_id, ctx->start_batch, ctx->end_batch);
    fflush(stdout);
    
    int batch_size = training->config.batch_size;
    int seq_len = training->config.sequence_length;
    int tokens_per_batch = batch_size * seq_len;
    
    // Allocate batch buffers
    uint32_t* input_tokens = (uint32_t*)malloc(tokens_per_batch * sizeof(uint32_t));
    uint32_t* target_tokens = (uint32_t*)malloc(tokens_per_batch * sizeof(uint32_t));
    
    if (!input_tokens || !target_tokens) {
        fprintf(stderr, "ERROR: Thread %d failed to allocate batch buffers\n", ctx->thread_id);
        free(input_tokens);
        free(target_tokens);
        return NULL;
    }
    
    printf("DEBUG: Thread %d allocated batch buffers\n", ctx->thread_id);
    fflush(stdout);
    
    // Allocate saved gradients structure
    SavedGradients saved = {0};
    
    float thread_loss = 0.0f;
    int batches_processed = 0;
    
    // Process assigned batches
    for (int batch_idx = ctx->start_batch; batch_idx < ctx->end_batch; batch_idx++) {
        printf("DEBUG: Thread %d processing batch %d\n", ctx->thread_id, batch_idx);
        fflush(stdout);
        
        // Get batch data
        int token_offset = batch_idx * tokens_per_batch;
        if (token_offset + tokens_per_batch > (int)training->num_tokens) {
            break;
        }
        
        // Copy tokens for this batch
        memcpy(input_tokens, &training->tokens[token_offset], tokens_per_batch * sizeof(uint32_t));
        memcpy(target_tokens, &training->tokens[token_offset + 1], (tokens_per_batch - 1) * sizeof(uint32_t));
        target_tokens[tokens_per_batch - 1] = training->tokens[token_offset]; // Wrap around
        
        // CRITICAL SECTION: Only one thread can do forward/backward at a time
        // because we're swapping pointers in the shared training structure
        pthread_mutex_lock(ctx->training_mutex);
        
        // Swap to thread-local gradient buffers
        swap_gradient_buffers(training, tg, &saved, 0);
        
        // Forward pass
        float loss = cllm_forward_training(training, input_tokens);
        
        // Compute loss
        loss += cllm_compute_loss_training(training, target_tokens);
        
        // Backward pass (writes to thread-local buffers)
        cllm_backward_training(training, target_tokens);
        
        // Restore original gradient buffers
        swap_gradient_buffers(training, tg, &saved, 1);
        
        pthread_mutex_unlock(ctx->training_mutex);
        
        thread_loss += loss;
        batches_processed++;
        
        printf("DEBUG: Thread %d completed batch %d, loss=%.4f\n", ctx->thread_id, batch_idx, loss);
        fflush(stdout);
    }
    
    // Store results
    ctx->total_loss = thread_loss;
    ctx->batches_completed = batches_processed;
    
    printf("DEBUG: Thread %d finished, processed %d batches, total_loss=%.4f\n", 
           ctx->thread_id, batches_processed, thread_loss);
    fflush(stdout);
    
    free(input_tokens);
    free(target_tokens);
    
    return NULL;
}

/**
 * Accumulate gradients from all threads
 */
static void accumulate_gradients(MTTrainingState* mt_state) {
    CLLMTraining* training = mt_state->training;
    CLLMModel* model = training->model;
    uint32_t vocab_size = model->vocab_size;
    uint32_t embed_dim = model->embedding_dim;
    uint32_t num_layers = model->num_layers;
    
    // Zero out main gradient buffers
    memset(training->gradients, 0, vocab_size * embed_dim * sizeof(float));
    
    for (uint32_t layer = 0; layer < num_layers; layer++) {
        uint64_t attn_size = embed_dim * embed_dim;
        memset(training->attention_grads[layer].query_lattice, 0, attn_size * sizeof(float));
        memset(training->attention_grads[layer].key_lattice, 0, attn_size * sizeof(float));
        memset(training->attention_grads[layer].value_lattice, 0, attn_size * sizeof(float));
        
        uint64_t ff_size = embed_dim * embed_dim;
        memset(training->ff_grads[layer].w1_lattice, 0, ff_size * sizeof(float));
        memset(training->ff_grads[layer].w2_lattice, 0, ff_size * sizeof(float));
        memset(training->ff_grads[layer].bias1, 0, embed_dim * sizeof(float));
        memset(training->ff_grads[layer].bias2, 0, embed_dim * sizeof(float));
        
        memset(training->ln_grads[layer].gamma, 0, embed_dim * sizeof(float));
        memset(training->ln_grads[layer].beta, 0, embed_dim * sizeof(float));
    }
    
    // Accumulate from all threads
    for (int t = 0; t < mt_state->num_threads; t++) {
        ThreadGradients* tg = mt_state->thread_contexts[t].thread_grads;
        
        // Accumulate embedding gradients
        for (size_t i = 0; i < vocab_size * embed_dim; i++) {
            training->gradients[i] += tg->embedding_grads[i];
        }
        
        // Accumulate layer gradients
        for (uint32_t layer = 0; layer < num_layers; layer++) {
            uint64_t attn_size = embed_dim * embed_dim;
            for (uint64_t i = 0; i < attn_size; i++) {
                training->attention_grads[layer].query_lattice[i] += tg->attention_grads[layer].query_lattice[i];
                training->attention_grads[layer].key_lattice[i] += tg->attention_grads[layer].key_lattice[i];
                training->attention_grads[layer].value_lattice[i] += tg->attention_grads[layer].value_lattice[i];
            }
            
            uint64_t ff_size = embed_dim * embed_dim;
            for (uint64_t i = 0; i < ff_size; i++) {
                training->ff_grads[layer].w1_lattice[i] += tg->ff_grads[layer].w1_lattice[i];
                training->ff_grads[layer].w2_lattice[i] += tg->ff_grads[layer].w2_lattice[i];
            }
            for (uint32_t i = 0; i < embed_dim; i++) {
                training->ff_grads[layer].bias1[i] += tg->ff_grads[layer].bias1[i];
                training->ff_grads[layer].bias2[i] += tg->ff_grads[layer].bias2[i];
            }
            
            for (uint32_t i = 0; i < embed_dim; i++) {
                training->ln_grads[layer].gamma[i] += tg->ln_grads[layer].gamma[i];
                training->ln_grads[layer].beta[i] += tg->ln_grads[layer].beta[i];
            }
        }
    }
    
    // Average gradients
    int total_batches = mt_state->total_batches_processed;
    if (total_batches > 0) {
        float scale = 1.0f / total_batches;
        
        for (size_t i = 0; i < vocab_size * embed_dim; i++) {
            training->gradients[i] *= scale;
        }
        
        for (uint32_t layer = 0; layer < num_layers; layer++) {
            uint64_t attn_size = embed_dim * embed_dim;
            for (uint64_t i = 0; i < attn_size; i++) {
                training->attention_grads[layer].query_lattice[i] *= scale;
                training->attention_grads[layer].key_lattice[i] *= scale;
                training->attention_grads[layer].value_lattice[i] *= scale;
            }
            
            uint64_t ff_size = embed_dim * embed_dim;
            for (uint64_t i = 0; i < ff_size; i++) {
                training->ff_grads[layer].w1_lattice[i] *= scale;
                training->ff_grads[layer].w2_lattice[i] *= scale;
            }
            for (uint32_t i = 0; i < embed_dim; i++) {
                training->ff_grads[layer].bias1[i] *= scale;
                training->ff_grads[layer].bias2[i] *= scale;
            }
            
            for (uint32_t i = 0; i < embed_dim; i++) {
                training->ln_grads[layer].gamma[i] *= scale;
                training->ln_grads[layer].beta[i] *= scale;
            }
        }
    }
}

/**
 * Multi-threaded training epoch
 */
float cllm_train_epoch_mt(CLLMTraining* training, int num_threads) {
    printf("DEBUG: cllm_train_epoch_mt called with %d threads\n", num_threads);
    fflush(stdout);
    
    if (!training || num_threads <= 0) return 0.0f;
    
    printf("DEBUG: Training pointer valid\n");
    fflush(stdout);
    
    // Calculate total batches
    int batch_size = training->config.batch_size;
    int seq_len = training->config.sequence_length;
    int tokens_per_batch = batch_size * seq_len;
    int total_batches = training->num_tokens / tokens_per_batch;
    
    printf("DEBUG: batch_size=%d, seq_len=%d, tokens_per_batch=%d, total_batches=%d\n",
           batch_size, seq_len, tokens_per_batch, total_batches);
    fflush(stdout);
    
    if (total_batches == 0) {
        printf("Not enough tokens for even one batch!\n");
        return 0.0f;
    }
    
    printf("=== MULTI-THREADED TRAINING ===\n");
    printf("Threads: %d\n", num_threads);
    printf("Total batches: %d\n", total_batches);
    printf("Batches per thread: %d\n", (total_batches + num_threads - 1) / num_threads);
    fflush(stdout);
    
    // Initialize MT state
    MTTrainingState mt_state = {0};
    mt_state.training = training;
    mt_state.num_threads = num_threads;
    mt_state.total_batches = total_batches;
    
    pthread_mutex_init(&mt_state.mutex, NULL);
    
    // Allocate thread contexts
    printf("DEBUG: Allocating thread contexts for %d threads\n", num_threads);
    fflush(stdout);
    
    mt_state.thread_contexts = (ThreadContext*)calloc(num_threads, sizeof(ThreadContext));
    if (!mt_state.thread_contexts) {
        fprintf(stderr, "ERROR: Failed to allocate thread contexts\n");
        return 0.0f;
    }
    
    printf("DEBUG: Thread contexts allocated\n");
    fflush(stdout);
    
    // Allocate thread-local gradient buffers and assign work
    int batches_per_thread = (total_batches + num_threads - 1) / num_threads;
    
    printf("DEBUG: Allocating thread-local gradient buffers\n");
    fflush(stdout);
    
    for (int i = 0; i < num_threads; i++) {
        printf("DEBUG: Allocating gradients for thread %d\n", i);
        fflush(stdout);
        
        ThreadGradients* tg = allocate_thread_gradients(training->model);
        if (!tg) {
            fprintf(stderr, "ERROR: Failed to allocate thread gradients for thread %d\n", i);
            // Cleanup
            for (int j = 0; j < i; j++) {
                free_thread_gradients(mt_state.thread_contexts[j].thread_grads, training->model->num_layers);
            }
            free(mt_state.thread_contexts);
            return 0.0f;
        }
        
        printf("DEBUG: Thread %d gradients allocated\n", i);
        fflush(stdout);
        
        mt_state.thread_contexts[i].training = training;
        mt_state.thread_contexts[i].thread_grads = tg;
        mt_state.thread_contexts[i].thread_id = i;
        mt_state.thread_contexts[i].num_threads = num_threads;
        mt_state.thread_contexts[i].training_mutex = &mt_state.mutex;
        mt_state.thread_contexts[i].start_batch = i * batches_per_thread;
        mt_state.thread_contexts[i].end_batch = (i + 1) * batches_per_thread;
        if (mt_state.thread_contexts[i].end_batch > total_batches) {
            mt_state.thread_contexts[i].end_batch = total_batches;
        }
    }
    
    // Create worker threads
    pthread_t* threads = (pthread_t*)malloc(num_threads * sizeof(pthread_t));
    for (int i = 0; i < num_threads; i++) {
        pthread_create(&threads[i], NULL, worker_thread, &mt_state.thread_contexts[i]);
    }
    
    // Wait for all threads to complete
    for (int i = 0; i < num_threads; i++) {
        pthread_join(threads[i], NULL);
    }
    
    // Accumulate results
    mt_state.total_loss = 0.0f;
    mt_state.total_batches_processed = 0;
    for (int i = 0; i < num_threads; i++) {
        mt_state.total_loss += mt_state.thread_contexts[i].total_loss;
        mt_state.total_batches_processed += mt_state.thread_contexts[i].batches_completed;
    }
    
    // Accumulate gradients from all threads
    accumulate_gradients(&mt_state);
    
    // Single optimizer step with accumulated gradients
    cllm_optimizer_step(training);
    
    // Calculate average loss
    float avg_loss = mt_state.total_batches_processed > 0 
        ? mt_state.total_loss / mt_state.total_batches_processed 
        : 0.0f;
    
    printf("MT epoch complete: %d batches, avg loss = %.4f\n", 
           mt_state.total_batches_processed, avg_loss);
    
    // Cleanup
    for (int i = 0; i < num_threads; i++) {
        free_thread_gradients(mt_state.thread_contexts[i].thread_grads, training->model->num_layers);
    }
    free(mt_state.thread_contexts);
    free(threads);
    pthread_mutex_destroy(&mt_state.mutex);
    
    return avg_loss;
}