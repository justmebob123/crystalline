#ifndef CLLM_TRAINING_H
#define CLLM_TRAINING_H

/*
 * cllm_training.h - Auto-generated header file
 * Source: cllm_training.c
 */

#include <stdint.h>
#include <stdbool.h>
#include <stddef.h>
#include <time.h>
#include "prime_types.h"

/* Local includes */
#include "cllm.h"

/* Type definitions */

/*
 * CLLM Training Configuration
 */
typedef struct {
    float learning_rate;         // Learning rate
    int batch_size;              // Batch size
    int num_epochs;              // Number of training epochs
    int max_steps;               // Maximum training steps
    float weight_decay;          // Weight decay (L2 regularization)
    float gradient_clip;         // Gradient clipping threshold
    int warmup_steps;            // Learning rate warmup steps
    int save_interval;           // Save checkpoint every N steps
    int save_every;              // Alternative save interval
    int eval_interval;           // Evaluate every N steps
    int sequence_length;         // Sequence length for training
    char optimizer[32];          // Optimizer name (e.g., "adam", "sgd")
    
    // Learning rate scheduling
    char lr_scheduler[32];       // Scheduler type: "none", "linear", "cosine", "step"
    float lr_decay_factor;       // Decay factor for step scheduler (default: 0.1)
    int lr_decay_steps;          // Steps between decay for step scheduler (default: 1000)
    float min_lr;                // Minimum learning rate (default: 1e-6)
    
    // Gradient accumulation
    int gradient_accumulation_steps;  // Number of steps to accumulate gradients (default: 1)
    
    // Mixed precision training
    int use_mixed_precision;          // Enable FP16/FP32 mixed precision (default: 0)
    float loss_scale;                 // Loss scaling factor for FP16 (default: 1024.0)
    float loss_scale_growth;          // Growth factor for dynamic loss scaling (default: 2.0)
    float loss_scale_backoff;         // Backoff factor for dynamic loss scaling (default: 0.5)
    int loss_scale_window;            // Steps before increasing loss scale (default: 2000)
} CLLMTrainingConfig;

/*
 * CLLM Training State
 */
typedef struct {
    CLLMModel* model;            // Pointer to the model
    CLLMTrainingConfig config;   // Training configuration
    
    // Training data
    uint32_t* tokens;            // Training tokens
    size_t num_tokens;           // Number of tokens
    
    // Training state
    int current_epoch;           // Current epoch
    int current_step;            // Current training step
    float best_loss;             // Best validation loss
    float current_loss;          // Current training loss
    time_t start_time;           // Training start time
    
    // Gradient accumulation state
    int accumulation_step;       // Current accumulation step (0 to gradient_accumulation_steps-1)
    
    // Mixed precision training state
    float* master_weights;       // FP32 master copy of weights (for mixed precision)
    uint16_t* fp16_activations;  // FP16 activation buffer
    uint16_t* fp16_gradients;    // FP16 gradient buffer
    float current_loss_scale;    // Current dynamic loss scale
    int loss_scale_steps;        // Steps since last loss scale increase
    
    // Batch management
    int total_batches;           // Total number of batches
    int current_batch_offset;    // Current batch offset in tokens
    
    // Optimizer state
    float* gradients;            // Gradient buffer for embeddings
    float* optimizer_state;      // Optimizer state (e.g., momentum, variance)
    
    // Layer-specific gradient buffers
    struct {
        float* query_lattice;    // Gradients for query weights
        float* key_lattice;      // Gradients for key weights
        float* value_lattice;    // Gradients for value weights
    }* attention_grads;          // Array of num_layers
    
    struct {
        float* w1_lattice;       // Gradients for W1
        float* w2_lattice;       // Gradients for W2
        float* bias1;            // Gradients for bias1
        float* bias2;            // Gradients for bias2
    }* ff_grads;                 // Array of num_layers
    
    struct {
        float* gamma;            // Gradients for gamma
        float* beta;             // Gradients for beta
    }* ln_grads;                 // Array of num_layers
    
    // Pre-allocated backward pass buffers (OPTIMIZATION)
    float* backward_embeddings;      // Reusable embedding buffer
    float* backward_grad_output;     // Reusable gradient output buffer
    float* backward_layer_input;     // Reusable layer input buffer
    float* backward_layer_grad;      // Reusable layer gradient buffer
    float* backward_temp_grad;       // Reusable temporary gradient buffer
    size_t backward_buffer_size;     // Size of activation buffers
    
    // Embedding cache for batch processing (OPTIMIZATION)
    float* cached_input_embeddings;  // Cached input embeddings
    float* cached_target_embeddings; // Cached target embeddings
    int cached_batch_size;           // Size of embedding cache
    
    // Forward pass activation storage
    float* input_embeddings;         // Input embeddings [batch * seq * embed]
    float** layer_inputs;            // Per-layer inputs [num_layers][batch * seq * embed]
    float** attention_outputs;       // Per-layer attention outputs
    float** ff_outputs;              // Per-layer FF outputs
    float** layer_outputs;           // Per-layer final outputs
    float** ff_hidden;               // Per-layer FF hidden states
    float* final_hidden;             // Final hidden state
    float* logits;                   // Output logits [batch * seq * vocab]
} CLLMTraining;

/* Loss computation functions */
float cllm_compute_loss(CLLMTraining* training, uint32_t* input_tokens, uint32_t* target_tokens, int num_tokens);
float cllm_compute_cross_entropy_loss(float* logits, uint32_t target, int vocab_size);
void cllm_compute_loss_gradient(float* logits, uint32_t target, float* grad_output, int vocab_size);
float cllm_compute_batch_loss(float* logits, uint32_t* targets, int batch_size, int vocab_size);
float cllm_compute_perplexity(float loss);
float cllm_compute_label_smoothing_loss(float* logits, uint32_t target, int vocab_size, float smoothing);
float cllm_compute_kl_divergence(float* logits, float* target_dist, int vocab_size);
float cllm_compute_sequence_loss(float* logits, uint32_t* targets, int seq_len, int vocab_size);
float cllm_compute_accuracy(float* logits, uint32_t* targets, int batch_size, int vocab_size);
float cllm_compute_top_k_accuracy(float* logits, uint32_t* targets, int batch_size, int vocab_size, int k);

/* Optimizer functions */
void cllm_apply_gradient_clipping(float* gradients, size_t size, float max_norm);
void cllm_clip_gradients_by_value(float* gradients, size_t size, float clip_value);
/* Training initialization and cleanup */
CLLMTraining* cllm_training_init(CLLMModel* model, CLLMTrainingConfig* config);
void cllm_training_free(CLLMTraining* training);
void cllm_training_cleanup(CLLMTraining* training);

void cllm_adam_step(CLLMTraining* training, float learning_rate);
void cllm_sgd_momentum_step(CLLMTraining* training, float learning_rate, float momentum);
void cllm_update_learning_rate(CLLMTraining* training);
float cllm_get_learning_rate(CLLMTraining* training);
float cllm_get_learning_rate_step_decay(CLLMTraining* training, int* decay_steps, int num_decay_steps, float decay_factor);
float cllm_get_learning_rate_exponential(CLLMTraining* training, float decay_rate);
float cllm_get_learning_rate_polynomial(CLLMTraining* training, float power);
void cllm_print_lr_schedule(CLLMTraining* training);
int cllm_adam_init(CLLMTraining* training);
void cllm_apply_weight_decay(float* weights, size_t size, float weight_decay, float learning_rate);
void cllm_zero_gradients(float* gradients, size_t size);
float cllm_compute_gradient_norm(float* gradients, size_t size);
void cllm_accumulate_gradients(float* accumulated_grads, float* current_grads, size_t size);
void cllm_scale_gradients(float* gradients, size_t size, float scale);
int cllm_check_gradients_valid(float* gradients, size_t size);
void cllm_update_ema_weights(float* ema_weights, float* current_weights, size_t size, float decay);

/* Backward pass functions */
void cllm_layer_norm_backward(CLLMLayerNorm* ln, float* input, float* grad_output, float* grad_input, float* grad_gamma, float* grad_beta);
void cllm_feedforward_backward(FeedForwardLayer* layer, float* input, float* hidden, float* grad_output, float* grad_input, float* grad_w1, float* grad_w2, float* grad_b1, float* grad_b2);
void cllm_attention_backward(AttentionLayer* layer, float* input, float* grad_output, float* grad_input,
                            float* grad_query_weights, float* grad_key_weights, float* grad_value_weights,
                            int seq_len);
void cllm_embedding_backward(Embeddings* embeddings, uint32_t* token_ids, float* grad_output, float* grad_embeddings, int batch_size);
void cllm_transformer_layer_backward(CLLMTraining* training, int layer_idx, float* input, float* grad_output, float* grad_input, int seq_len);
void cllm_zero_all_gradients(CLLMTraining* training);
void cllm_backward(CLLMTraining* training, uint32_t* input_tokens, uint32_t* target_tokens, int num_tokens);
void cllm_zero_all_gradients(CLLMTraining* training);

/* Function declarations */
int cllm_load_training_data(CLLMTraining* training, const char* filename);
int cllm_get_batch(CLLMTraining* training, uint32_t* input_tokens, uint32_t* target_tokens);
void cllm_optimizer_step(CLLMTraining* training);
void cllm_optimizer_step_adam(CLLMTraining* training);  // Adam optimizer with gradient accumulation
float cllm_train_epoch(CLLMTraining* training);
int cllm_train(CLLMTraining* training);
int cllm_save_checkpoint(CLLMTraining* training, const char* filename);
int cllm_load_checkpoint(CLLMTraining* training, const char* filename);
void cllm_training_cleanup(CLLMTraining* training);

/* Training step functions (used by multi-threading and production features) */
float cllm_forward_training(CLLMTraining* training, uint32_t* input_tokens);
float cllm_compute_loss_training(CLLMTraining* training, uint32_t* target_tokens);
void cllm_backward_training(CLLMTraining* training, uint32_t* target_tokens);

#endif /* CLLM_TRAINING_H */