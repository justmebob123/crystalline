# SECONDARY OBJECTIVES - Crystalline Enhancement Plan

## Overview
This document outlines comprehensive enhancements to the Crystalline system, focusing on:
1. **CRITICAL: Dynamic Training System with Per-Model Queues** (NEW - HIGHEST PRIORITY)
2. **CRITICAL: Model Naming, Renaming, and Cross-Model Training** (NEW - HIGHEST PRIORITY)
3. **CRITICAL: Model Selection Across All Tabs** (NEW - HIGHEST PRIORITY)
4. **CRITICAL: Fix Rainbow Table to Use Clock Lattice Mapping** (HIGH PRIORITY)
5. Complete UI enhancements (all pending features)
6. Advanced crawler controls with prime-based randomization
7. Pure C implementation of all file processors (no Python dependencies)
8. Proper library organization and categorization
9. Advanced URL pattern detection
10. Dependency management for multiple Linux distributions

---

## üî¥ CRITICAL PRIORITY 1: DYNAMIC TRAINING SYSTEM WITH PER-MODEL QUEUES

### Overview
Implement a complete dynamic training system where each model has its own training queue, configuration, and epoch tracking. This replaces the current hardcoded single-model system.

### Current Problems
- ‚ùå Hardcoded batch_size=1, sequence_length=64, epochs=5
- ‚ùå Single global queue (crawler_data/tokenized/)
- ‚ùå No per-model configuration or tracking
- ‚ùå No batch accumulation (fails on small inputs)
- ‚ùå No epoch tracking or continuation
- ‚ùå No layer validation

### Required Implementation

#### 1.1 Enhanced Model Structure
**File:** `include/cllm.h`

Add new fields to CLLMModel:
```c
typedef struct {
    // Existing fields
    uint32_t vocab_size;
    uint32_t embedding_dim;
    uint32_t num_layers;         // FIXED - cannot be reduced
    uint32_t num_heads;
    uint32_t ff_dim;
    
    // Model weights (existing)
    float* embeddings;
    CLLMLayer* layers;
    float* output_weights;
    float* lattice_coords;
    uint32_t* symmetry_groups;
    
    // NEW: Training metadata
    char model_name[256];        // User-defined model name
    int epochs_trained;          // Total epochs trained so far
    int target_epochs;           // Target epochs for next training session
    char queue_directory[512];   // Path to model-specific queue
    CLLMTrainingConfig config;   // Per-model training configuration
    
    // NEW: Training history
    float* loss_history;         // Loss per epoch
    int history_size;            // Number of recorded epochs
} CLLMModel;
```

#### 1.2 Per-Model Queue System
**Files:** `include/ai/model_queue_manager.h`, `src/ai/model_queue_manager.c`

```c
// Create queue for model
int create_model_queue(const char* model_name);

// Add file to model's queue
int add_to_model_queue(const char* model_name, const char* tokenized_file);

// Get next file from model's queue
char* get_next_from_queue(const char* model_name);

// Get queue size
int get_queue_size(const char* model_name);

// Clear queue
int clear_model_queue(const char* model_name);

// NEW: Import queue from another model (cross-model training)
int import_queue_from_model(const char* target_model, const char* source_model);
```

**Directory Structure:**
```
models/
    ‚îú‚îÄ‚îÄ model1.cllm
    ‚îú‚îÄ‚îÄ model1_queue/
    ‚îÇ   ‚îú‚îÄ‚îÄ file1.txt (tokenized)
    ‚îÇ   ‚îú‚îÄ‚îÄ file2.txt (tokenized)
    ‚îÇ   ‚îî‚îÄ‚îÄ file3.txt (tokenized)
    ‚îú‚îÄ‚îÄ model2.cllm
    ‚îú‚îÄ‚îÄ model2_queue/
    ‚îÇ   ‚îú‚îÄ‚îÄ file1.txt (tokenized)
    ‚îÇ   ‚îî‚îÄ‚îÄ file2.txt (tokenized)
    ‚îî‚îÄ‚îÄ crawler_model.cllm
        ‚îî‚îÄ‚îÄ crawler_model_queue/
            ‚îú‚îÄ‚îÄ crawled1.txt (tokenized)
            ‚îî‚îÄ‚îÄ crawled2.txt (tokenized)
```

#### 1.3 Batch Accumulation System
**Files:** `include/ai/batch_accumulator.h`, `src/ai/batch_accumulator.c`

```c
typedef struct {
    uint32_t* token_buffer;      // Accumulated tokens
    size_t buffer_size;          // Current buffer size
    size_t buffer_capacity;      // Maximum buffer capacity
    int target_batch_size;       // Target batch size
    int sequence_length;         // Sequence length
    pthread_mutex_t lock;        // Thread safety
} BatchAccumulator;

// Initialize accumulator
BatchAccumulator* batch_accumulator_create(int batch_size, int seq_len);

// Add tokens to accumulator
int batch_accumulator_add(BatchAccumulator* acc, uint32_t* tokens, size_t num_tokens);

// Check if ready for training
int batch_accumulator_ready(BatchAccumulator* acc);

// Get batch for training
int batch_accumulator_get_batch(BatchAccumulator* acc, uint32_t** batch_tokens, size_t* batch_size);

// Clear accumulator
void batch_accumulator_clear(BatchAccumulator* acc);
```

**Example Flow:**
```
File 1: 50 tokens ‚Üí Buffer: 50 (not ready)
File 2: 30 tokens ‚Üí Buffer: 80 (not ready)
File 3: 80 tokens ‚Üí Buffer: 160 (not ready)
File 4: 100 tokens ‚Üí Buffer: 260 (READY!)
  ‚Üí Create batch with 256 tokens
  ‚Üí Train model
  ‚Üí Buffer: 4 tokens remaining
```

#### 1.4 Epoch Tracking System
**File:** `src/ai/cllm_model_manager.c`

```c
// Get epochs trained
int cllm_model_get_epochs_trained(CLLMModel* model);

// Set target epochs
int cllm_model_set_target_epochs(CLLMModel* model, int target);

// Validate epoch configuration
int cllm_model_validate_epochs(CLLMModel* model, int target_epochs);

// Continue training from checkpoint
int cllm_model_continue_training(CLLMModel* model, int additional_epochs);
```

**Example:**
```
Initial: epochs_trained=0, target_epochs=10
  ‚Üí Train for 10 epochs
  ‚Üí epochs_trained=10

Later: epochs_trained=10, target_epochs=20
  ‚Üí Train for 10 more epochs
  ‚Üí epochs_trained=20

Cannot: epochs_trained=20, target_epochs=15
  ‚Üí ERROR: Cannot reduce epochs (minimum is 20)
```

#### 1.5 Layer Validation System
**File:** `src/ai/cllm_model_manager.c`

```c
// Validate layer configuration
int cllm_model_validate_layers(CLLMModel* existing_model, int new_layers);

// Check if layer reduction
int cllm_model_is_layer_reduction(CLLMModel* model, int new_layers);

// Increase layers (with proper initialization)
int cllm_model_increase_layers(CLLMModel* model, int new_layers);

// Get layer constraints
typedef struct {
    int min_layers;      // Minimum layers (from existing model)
    int max_layers;      // Maximum layers (system limit)
    int current_layers;  // Current layers
    int can_reduce;      // 0 = cannot reduce, 1 = can reduce (new model)
} LayerConstraints;

LayerConstraints cllm_model_get_layer_constraints(CLLMModel* model);
```

### Priority: HIGHEST
This is the foundation for all dynamic training features.

---

## üî¥ CRITICAL PRIORITY 2: MODEL NAMING, RENAMING, AND CROSS-MODEL TRAINING

### Overview
Allow users to name and rename models with automatic file and directory renaming. Enable cross-model training where one model can train on another model's data.

### Required Implementation

#### 2.1 Model Naming System
**File:** `src/ai/cllm_model_manager.c`

```c
// Set model name
int cllm_model_set_name(CLLMModel* model, const char* name);

// Get model name
const char* cllm_model_get_name(CLLMModel* model);

// Validate model name (unique, valid characters)
int cllm_model_validate_name(const char* name);

// Generate default name (model_1, model_2, etc.)
char* cllm_model_generate_default_name(void);
```

#### 2.2 Model Renaming System
**File:** `src/ai/cllm_model_manager.c`

```c
// Rename model (renames file and queue directory)
int model_manager_rename_model(const char* old_name, const char* new_name);

// Implementation:
// 1. Validate new name is unique
// 2. Rename model file: models/old_name.cllm ‚Üí models/new_name.cllm
// 3. Rename queue directory: models/old_name_queue/ ‚Üí models/new_name_queue/
// 4. Update model->model_name field
// 5. Update model->queue_directory field
// 6. Save model with new name
// 7. Delete old model file
```

**Example:**
```
Before:
  models/model_1.cllm
  models/model_1_queue/

Rename "model_1" to "research_papers":
  models/research_papers.cllm
  models/research_papers_queue/
```

#### 2.3 Cross-Model Training System
**File:** `src/ai/model_queue_manager.c`

```c
// Import training data from another model's queue
int import_queue_from_model(const char* target_model, const char* source_model);

// Implementation:
// 1. Get source model's queue directory
// 2. Copy all tokenized files to target model's queue
// 3. Maintain source files (don't move, copy)
// 4. Update target model's queue size
// 5. Log import operation
```

**Example:**
```
Training Tab:
  Selected Model: model_A
  Import Data From: [dropdown: model_B, model_C, crawler_model]
  [Import Data Button]
  
Result:
  model_A_queue/ now contains copies of model_B_queue/ files
  model_A can train on model_B's data
  model_B's queue unchanged
```

### Priority: HIGHEST
Essential for flexible model management and cross-model learning.

---

## üî¥ CRITICAL PRIORITY 3: MODEL SELECTION ACROSS ALL TABS

### Overview
Every tab should have a model selector dropdown. Main model controls are in Models tab, but all tabs can select and use any loaded model.

### Required Implementation

#### 3.1 Global Model Selection State
**File:** `app/main.c`

```c
typedef struct {
    char selected_model_training[256];   // Training tab selection
    char selected_model_crawler[256];    // Crawler tab selection
    char selected_model_llm[256];        // LLM tab selection
    char selected_model_research[256];   // Research tab selection
    pthread_mutex_t lock;                // Thread safety
} GlobalModelSelection;

GlobalModelSelection g_model_selection = {0};
```

#### 3.2 Model Selector UI Component
**File:** `app/ui/model_selector.c` (NEW)

```c
typedef struct {
    int x, y, width, height;
    char selected_model[256];
    char** model_list;
    int num_models;
    int dropdown_open;
    void (*on_change)(const char* model_name);
} ModelSelector;

// Create model selector
ModelSelector* model_selector_create(int x, int y, int width, int height);

// Render model selector
void model_selector_render(ModelSelector* selector);

// Handle click
int model_selector_handle_click(ModelSelector* selector, int mouse_x, int mouse_y);

// Update model list
void model_selector_update_list(ModelSelector* selector);

// Get selected model
const char* model_selector_get_selected(ModelSelector* selector);
```

#### 3.3 Integration with All Tabs

**Training Tab** (`app/ui/tabs/tab_training.c`):
```c
// Add model selector at top
ModelSelector* training_model_selector;

// Add "Import Data From" selector
ModelSelector* import_data_selector;

// Render both selectors
model_selector_render(training_model_selector);
model_selector_render(import_data_selector);

// When model selected, update configuration display
void on_training_model_change(const char* model_name) {
    // Load model configuration
    // Update UI with model's batch_size, epochs, etc.
    // Update queue size display
}
```

**Crawler Tab** (`app/ui/tabs/tab_crawler.c`):
```c
// Add model selector at top
ModelSelector* crawler_model_selector;

// Default to "crawler_model" but allow selection
// When model selected, crawler writes to that model's queue
```

**LLM Tab** (`app/ui/tabs/tab_llm.c`):
```c
// Add model selector at top
ModelSelector* llm_model_selector;

// When model selected, use that model for inference
```

**Research Tab** (`app/ui/tabs/tab_research.c`):
```c
// Add model selector at top
ModelSelector* research_model_selector;

// When model selected, use that model for research tasks
```

**Models Tab** (`app/ui/tabs/tab_models.c`):
```c
// Main model management controls
// - Create new model (with name input)
// - Rename existing model
// - Delete model
// - View model details
// - Model list with all loaded models
```

### UI Layout Example

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Training Tab                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Model: [dropdown: model_1, model_2, ...]‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ Configuration:                          ‚îÇ
‚îÇ   Batch Size: [slider: 1-32] [4]      ‚îÇ
‚îÇ   Sequence Length: [slider: 32-512] [64]‚îÇ
‚îÇ   Epochs: [slider: 1-100] [10]        ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ Current Model Info:                     ‚îÇ
‚îÇ   Name: model_1                        ‚îÇ
‚îÇ   Layers: 6 (cannot reduce)            ‚îÇ
‚îÇ   Epochs Trained: 15                   ‚îÇ
‚îÇ   Target Epochs: 25 (will train 10 more)‚îÇ
‚îÇ   Queue Size: 42 files                 ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ Import Data From: [dropdown: model_2, ...]‚îÇ
‚îÇ [Import Data] [Start Training] [Stop]  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Priority: HIGHEST
Essential for multi-model workflow and user flexibility.

---

## üî¥ CRITICAL PRIORITY: FIX RAINBOW TABLE COORDINATE SYSTEM

### Problem Identified
The `prime_rainbow.c` file is using **SPIRAL COORDINATE FORMULAS** instead of the **CLOCK LATTICE MAPPING** implemented in `clock_lattice.c`. This causes the X-pattern visualization instead of the correct concentric ring structure.

### Current INCORRECT Implementation (prime_rainbow.c)

```c
// ‚ùå WRONG - Uses spiral formula
double fast_prime_angle(int prime) {
    return (2.0 * PRIME_PI * prime) / (prime + 1);  // Spiral angle
}

// ‚ùå WRONG - Radius grows outward with sqrt(prime)
double fast_prime_radius(int prime) {
    return prime_sqrt((double)prime);  // Spiral radius
}

// ‚ùå WRONG - No relationship to Babylonian structure
int fast_prime_layer(int prime) {
    return (prime % RAINBOW_LAYERS) + 1;  // Arbitrary layering
}

// ‚ùå WRONG - Uses spiral coordinates
void fast_prime_fold_coords(int prime, double* x, double* y) {
    double angle = fast_prime_angle(prime);
    double radius = fast_prime_radius(prime);
    *x = radius * prime_cos(angle);
    *y = radius * prime_sin(angle);
}
```

### Correct Implementation (Should Use clock_lattice.c)

```c
// ‚úÖ CORRECT - Use clock lattice mapping
double fast_prime_angle(int prime_index) {
    BabylonianClockPosition pos = map_prime_index_to_clock(prime_index);
    return pos.angle;
}

// ‚úÖ CORRECT - Use clock lattice radius (INWARD counting)
double fast_prime_radius(int prime_index) {
    BabylonianClockPosition pos = map_prime_index_to_clock(prime_index);
    return pos.radius;  // 0.25 (outer) to 1.0 (inner)
}

// ‚úÖ CORRECT - Use clock lattice ring
int fast_prime_layer(int prime_index) {
    BabylonianClockPosition pos = map_prime_index_to_clock(prime_index);
    return pos.ring;  // 0-3 for first 232 primes
}

// ‚úÖ CORRECT - Use clock lattice then fold to sphere
void fast_prime_fold_coords(int prime_index, double* x, double* y, double* z) {
    BabylonianClockPosition pos = map_prime_index_to_clock(prime_index);
    SphereCoord sphere = fold_clock_to_sphere(pos);
    *x = sphere.x;
    *y = sphere.y;
    *z = sphere.z;
}
```

### Mathematical Foundation

**Babylonian Clock Structure (from MASTER_PLAN OBJECTIVE 19):**
```
Ring 0 (OUTSIDE): ‚àû/0 boundary
  ‚Üì (counting INWARD toward unity)
Ring 1: Primes 1-12 (HOURS) at 25% radius - OUTER (smaller primes)
  ‚Üì
Ring 2: Primes 13-72 (MINUTES) at 50% radius
  ‚Üì
Ring 3: Primes 73-132 (SECONDS) at 75% radius
  ‚Üì
Ring 4: Primes 133-232 (MILLISECONDS) at 100% radius - INNER (larger primes)
  ‚Üì
Center: 1 (Unity)
```

**Key Principles:**
1. **Smaller primes on OUTER rings** (counting inward from ‚àû/0)
2. **Larger primes on INNER rings** (approaching unity at center)
3. **Clock positions, NOT spiral** (12, 60, 60, 100 positions)
4. **Modular arithmetic** (mod 12, mod 60, mod 100)
5. **12-fold symmetry** (primes > 3 in {1, 5, 7, 11} mod 12)
6. **Stereographic projection** to fold clock ‚Üí sphere

### Implementation Steps

#### Step 1: Update Function Signatures
Change from `int prime` to `int prime_index` to clarify we're using prime INDEX not prime VALUE:

```c
// OLD signatures (confusing):
double fast_prime_angle(int prime);
double fast_prime_radius(int prime);

// NEW signatures (clear):
double fast_prime_angle(int prime_index);
double fast_prime_radius(int prime_index);
```

#### Step 2: Include clock_lattice.h
```c
#include "clock_lattice.h"
```

#### Step 3: Rewrite Each Function

**fast_prime_angle():**
```c
double fast_prime_angle(int prime_index) {
    BabylonianClockPosition pos = map_prime_index_to_clock(prime_index);
    return pos.angle;
}
```

**fast_prime_radius():**
```c
double fast_prime_radius(int prime_index) {
    BabylonianClockPosition pos = map_prime_index_to_clock(prime_index);
    return pos.radius;
}
```

**fast_prime_layer():**
```c
int fast_prime_layer(int prime_index) {
    BabylonianClockPosition pos = map_prime_index_to_clock(prime_index);
    return pos.ring;
}
```

**fast_prime_fold_coords():**
```c
void fast_prime_fold_coords(int prime_index, double* x, double* y, double* z) {
    if (!x || !y || !z) return;
    
    // Get clock position
    BabylonianClockPosition pos = map_prime_index_to_clock(prime_index);
    
    // Fold to 3D sphere
    SphereCoord sphere = fold_clock_to_sphere(pos);
    
    *x = sphere.x;
    *y = sphere.y;
    *z = sphere.z;
}
```

**fast_prime_position():**
```c
int fast_prime_position(int prime_index) {
    BabylonianClockPosition pos = map_prime_index_to_clock(prime_index);
    return pos.position;
}
```

#### Step 4: Update BigInt Versions

```c
double big_fast_prime_angle(BigInt *prime) {
    if (!prime || prime->len == 0) return 0.0;
    
    // Convert BigInt to prime index (this needs proper implementation)
    // For now, use first digit as approximation
    uint64_t prime_index = prime->d[0];
    return fast_prime_angle((int)prime_index);
}

// Similar for other big_fast_prime_* functions
```

#### Step 5: Add Modular Arithmetic Functions

```c
// Get modular relationships for a prime
PrimeModular fast_prime_modular(uint64_t prime) {
    return get_prime_modular(prime);  // Use clock_lattice function
}

// Check if position is sacred (œÄ, 12 o'clock, etc.)
bool fast_prime_is_sacred(int prime_index) {
    BabylonianClockPosition pos = map_prime_index_to_clock(prime_index);
    return is_sacred_position(pos);  // Use clock_lattice function
}
```

#### Step 6: Update Frequency Function

```c
double fast_prime_frequency(int prime_index) {
    // Base frequency on clock position, not prime value
    BabylonianClockPosition pos = map_prime_index_to_clock(prime_index);
    
    // Frequency based on ring (deeper rings = higher frequency)
    double base_freq = 432.0;  // A4 tuning
    double ring_multiplier = 1.0 + (pos.ring * 0.5);  // Ring 0=1.0, Ring 3=2.5
    
    // Modulate by position within ring
    double position_factor = (double)pos.position / 12.0;  // Normalized
    
    return base_freq * ring_multiplier * (1.0 + position_factor * PHI);
}
```

### Testing Requirements

After implementation, verify:

1. **Visual Test**: Visualization shows concentric rings, NOT X pattern
2. **Ring Test**: Primes 1-12 on outer ring (25% radius)
3. **Ring Test**: Primes 133-232 on inner ring (100% radius)
4. **Angle Test**: 3 o'clock position = 0¬∞ (œÄ position)
5. **Angle Test**: 12 o'clock position = -90¬∞ (top)
6. **Symmetry Test**: All primes > 3 in {1, 5, 7, 11} mod 12
7. **Modular Test**: mod_12, mod_60, mod_100 relationships correct
8. **Sphere Test**: Stereographic projection produces unit sphere

### Files to Modify

1. **src/geometry/prime_rainbow.c** - Rewrite coordinate functions
2. **include/prime_rainbow.h** - Update function signatures
3. **app/visualization.c** (if exists) - Update visualization to use new coords
4. **tests/** - Add tests for clock lattice integration

### Dependencies

- `clock_lattice.h` - Already implemented correctly
- `clock_lattice.c` - Already implemented correctly
- No new dependencies needed

### Expected Results

After fix:
- ‚úÖ Concentric ring visualization (not X pattern)
- ‚úÖ Smaller primes on outer rings
- ‚úÖ Larger primes on inner rings
- ‚úÖ Clock positions (12, 60, 60, 100)
- ‚úÖ Proper stereographic projection
- ‚úÖ 12-fold symmetry maintained
- ‚úÖ Modular arithmetic relationships preserved

### Priority: HIGH
This must be fixed before the rainbow table can be used for proper prime visualization and semantic embedding.

---

---

## RULES & STRUCTURE

### Core Principles
1. **Pure C Implementation**: All code must be written in C, no Python scripts
2. **Library Organization**: Proper categorization and modular design
3. **External Dependencies**: Document all dependencies with installation scripts
4. **Custom Implementations**: Provide C implementations where libraries don't exist
5. **UI Integration**: All features must have proper UI controls
6. **Testing**: Each component must be tested before integration
7. **Documentation**: Comprehensive docs for all new features

### Implementation Order
1. Library reorganization (foundation)
2. File processor C implementations (core functionality)
3. Advanced crawler features (prime randomization, URL patterns)
4. UI enhancements (all pending features)
5. Testing and integration
6. Documentation and dependency management

---

## PHASE 1: LIBRARY REORGANIZATION (4 hours)

### 1.1 Create Unified Document Processing Library (2 hours)
**Goal**: Consolidate all file processing into `libdocproc`

#### Structure
```
src/docproc/
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ format_detection.c      # File type detection
‚îÇ   ‚îú‚îÄ‚îÄ text_extraction.c       # Base extraction functions
‚îÇ   ‚îî‚îÄ‚îÄ text_cleaning.c         # Text normalization
‚îú‚îÄ‚îÄ formats/
‚îÇ   ‚îú‚îÄ‚îÄ pdf_processor.c         # PDF extraction (poppler)
‚îÇ   ‚îú‚îÄ‚îÄ office_processor.c      # DOC/DOCX/RTF (custom + antiword)
‚îÇ   ‚îú‚îÄ‚îÄ spreadsheet_processor.c # XLS/XLSX (custom implementation)
‚îÇ   ‚îú‚îÄ‚îÄ presentation_processor.c # PPT/PPTX (custom implementation)
‚îÇ   ‚îú‚îÄ‚îÄ odf_processor.c         # ODT/ODS/ODP (XML parsing)
‚îÇ   ‚îú‚îÄ‚îÄ ebook_processor.c       # EPUB (ZIP + HTML)
‚îÇ   ‚îú‚îÄ‚îÄ markup_processor.c      # MD/HTML/XML
‚îÇ   ‚îú‚îÄ‚îÄ data_processor.c        # JSON/CSV/YAML/TOML
‚îÇ   ‚îú‚îÄ‚îÄ code_processor.c        # Source code files
‚îÇ   ‚îú‚îÄ‚îÄ email_processor.c       # EML/MSG
‚îÇ   ‚îú‚îÄ‚îÄ image_processor.c       # OCR (tesseract)
‚îÇ   ‚îî‚îÄ‚îÄ archive_processor.c     # ZIP/TAR/etc
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ xml_parser.c            # Custom XML parser
‚îÇ   ‚îú‚îÄ‚îÄ zip_handler.c           # ZIP file handling
‚îÇ   ‚îî‚îÄ‚îÄ encoding_converter.c    # Character encoding
‚îî‚îÄ‚îÄ docproc.h                   # Public API
```

#### Tasks
- [ ] Create directory structure
- [ ] Design unified API
- [ ] Implement format detection in pure C
- [ ] Create base extraction framework
- [ ] Add error handling and logging

### 1.2 Implement Custom File Format Processors (2 hours)
**Goal**: Pure C implementations for all formats

#### Priority Formats (Custom C Implementation Required)
- [ ] **DOCX/XLSX/PPTX**: ZIP + XML parsing (no python-docx)
- [ ] **ODF (ODT/ODS/ODP)**: ZIP + XML parsing (no odt2txt)
- [ ] **EPUB**: ZIP + HTML parsing (no ebooklib)
- [ ] **YAML**: Custom YAML parser in C
- [ ] **TOML**: Custom TOML parser in C

#### Formats Using Existing C Libraries
- [ ] **PDF**: pdftotext (poppler-utils)
- [ ] **DOC**: antiword
- [ ] **RTF**: unrtf
- [ ] **Images**: tesseract
- [ ] **JSON**: jq or custom parser
- [ ] **CSV**: Custom parser (simple)

---

## PHASE 2: PURE C FILE PROCESSORS (8 hours)

### 2.1 Office Open XML (DOCX/XLSX/PPTX) Processor (3 hours)
**Implementation**: ZIP extraction + XML parsing

#### Components
- [ ] ZIP file reader (libzip or custom)
- [ ] XML parser (libxml2 or custom)
- [ ] DOCX text extraction
  - Parse document.xml
  - Extract paragraphs, tables, headers
  - Handle styles and formatting
- [ ] XLSX data extraction
  - Parse worksheets
  - Extract cell values
  - Handle formulas
- [ ] PPTX slide extraction
  - Parse slides
  - Extract text from shapes
  - Handle notes and comments

#### Files to Create
```c
src/docproc/formats/ooxml_processor.c
src/docproc/formats/ooxml_parser.c
src/docproc/utils/zip_handler.c
```

### 2.2 OpenDocument Format (ODT/ODS/ODP) Processor (2 hours)
**Implementation**: ZIP extraction + XML parsing

#### Components
- [ ] ODF structure parser
- [ ] ODT text extraction (content.xml)
- [ ] ODS spreadsheet extraction
- [ ] ODP presentation extraction

#### Files to Create
```c
src/docproc/formats/odf_processor.c
```

### 2.3 EPUB Processor (1 hour)
**Implementation**: ZIP extraction + HTML parsing

#### Components
- [ ] EPUB structure parser (container.xml, content.opf)
- [ ] Chapter extraction
- [ ] HTML content parsing
- [ ] Metadata extraction

#### Files to Create
```c
src/docproc/formats/epub_processor.c
```

### 2.4 YAML/TOML Parsers (2 hours)
**Implementation**: Custom parsers in C

#### YAML Parser
- [ ] Tokenizer
- [ ] Parser for basic YAML structures
- [ ] Value extraction
- [ ] Text conversion

#### TOML Parser
- [ ] INI-style parser
- [ ] Section handling
- [ ] Value extraction

#### Files to Create
```c
src/docproc/formats/yaml_parser.c
src/docproc/formats/toml_parser.c
```

---

## PHASE 3: ADVANCED CRAWLER FEATURES (20 hours)

### 3.1 Prime-Based Randomization System ‚úÖ COMPLETE
**Time: 2 hours**

#### Features
- [x] Prime number selection for crawl frequency
- [x] Prime-based delay calculation
- [x] Prime-based link selection from queue
- [x] Configurable prime ranges

#### Status
- Implementation complete in `src/crawler/prime_randomization.c`
- Integrated with crawler core
- Using prime 13 for URL selection

---

### 3.2 Robots.txt Support üî¥ HIGH PRIORITY
**Time: 2 hours**

#### Purpose
Respect website crawling policies and avoid getting blocked.

#### Features
- [ ] Download robots.txt for each domain
- [ ] Parse User-agent and Disallow directives
- [ ] Check URLs against rules before crawling
- [ ] Respect Crawl-delay directive
- [ ] Cache robots.txt per domain (24 hour TTL)

#### Implementation
```c
typedef struct {
    char domain[256];
    char** disallowed_paths;
    int num_disallowed;
    int crawl_delay;
    time_t cache_time;
} RobotsTxt;

bool robots_txt_allows(const char* url);
int robots_txt_get_delay(const char* domain);
```

#### Files to Create
```c
src/crawler/robots_txt.c
src/crawler/robots_txt.h
```

#### Benefits
- Ethical crawling
- Avoid getting blocked
- Respect server resources
- Legal compliance

---

### 3.3 Per-Domain Rate Limiting üî¥ HIGH PRIORITY
**Time: 1 hour**

#### Purpose
Prevent hammering single domain and avoid rate limiting.

#### Features
- [ ] Track last request time per domain
- [ ] Enforce minimum delay per domain (default: 10s)
- [ ] Queue requests to same domain
- [ ] Distribute requests across domains

#### Implementation
```c
typedef struct {
    char domain[256];
    time_t last_request;
    int request_count;
    int delay_seconds;
} DomainRateLimit;

bool domain_rate_limit_check(const char* domain);
void domain_rate_limit_update(const char* domain);
```

#### Benefits
- Avoid rate limiting
- Better server citizenship
- More reliable crawling
- Reduced blocking risk

---

### 3.4 Content Quality Filtering üî¥ HIGH PRIORITY
**Time: 2 hours**

#### Purpose
Skip low-quality or problematic pages to improve training data quality.

#### Detection Patterns
- [ ] **Error Pages:** 404, 500, 503 status codes
- [ ] **Login Pages:** "login", "signin", "password" in URL or content
- [ ] **Paywall Pages:** "subscribe", "premium", "paywall" markers
- [ ] **Duplicate Content:** Hash-based deduplication
- [ ] **Low Quality:** Very short content, excessive ads, spam patterns

#### Implementation
```c
typedef enum {
    QUALITY_HIGH,
    QUALITY_MEDIUM,
    QUALITY_LOW,
    QUALITY_SKIP
} ContentQuality;

ContentQuality assess_content_quality(const char* html, const char* url);
bool should_skip_content(ContentQuality quality);
```

#### Benefits
- Better training data quality
- Reduced storage waste
- Faster processing
- Improved model performance

---

### 3.5 Advanced URL Pattern Detection üü° MEDIUM PRIORITY
**Time: 2 hours**

#### Purpose
Extract more URLs from complex pages.

#### Patterns to Detect
- [ ] JavaScript onclick handlers: `onclick="location.href='...'"`
- [ ] Data attributes: `data-href="..."`, `data-url="..."`
- [ ] Meta refresh: `<meta http-equiv="refresh" content="0;url=...">`
- [ ] JavaScript redirects: `window.location = "..."`
- [ ] AJAX endpoints: API calls in JavaScript
- [ ] Sitemap.xml references
- [ ] RSS/Atom feeds
- [ ] Canonical URLs: `<link rel="canonical" href="...">`
- [ ] Open Graph URLs: `<meta property="og:url" content="...">`

#### Implementation
```c
typedef enum {
    URL_PATTERN_HREF,
    URL_PATTERN_ONCLICK,
    URL_PATTERN_DATA_ATTR,
    URL_PATTERN_META_REFRESH,
    URL_PATTERN_JS_LOCATION,
    URL_PATTERN_AJAX,
    URL_PATTERN_API,
    URL_PATTERN_SITEMAP,
    URL_PATTERN_FEED,
    URL_PATTERN_CANONICAL,
    URL_PATTERN_OPENGRAPH
} URLPatternType;

int extract_urls_advanced(const char* html, const char* base_url, 
                         URLPatternType* patterns, int num_patterns,
                         const char* output_file);
```

#### Files to Create
```c
src/crawler/url_patterns.c
src/crawler/url_patterns.h
```

#### Benefits
- More comprehensive crawling
- Discover hidden content
- Better site coverage
- Find structured data

---

### 3.6 Duplicate Content Detection üü° MEDIUM PRIORITY
**Time: 2 hours**

#### Purpose
Avoid processing same content multiple times.

#### Features
- [ ] Hash-based deduplication (SHA-256)
- [ ] Store hashes in database or file
- [ ] Check before processing
- [ ] Handle near-duplicates (fuzzy matching)

#### Implementation
```c
typedef struct {
    uint8_t hash[32];  // SHA-256
    char url[2048];
    time_t timestamp;
} ContentHash;

bool is_duplicate_content(const char* content);
void store_content_hash(const char* content, const char* url);
```

#### Benefits
- Reduced storage
- Faster processing
- Better training data diversity
- Avoid redundant work

---

### 3.7 Crawl Depth Control üü° MEDIUM PRIORITY
**Time: 1 hour**

#### Purpose
Limit how deep crawler goes into site.

#### Features
- [ ] Track depth from start URL
- [ ] Configurable max depth (default: 3)
- [ ] Breadth-first vs depth-first option
- [ ] Priority queue for important pages

#### Implementation
```c
typedef struct {
    char url[2048];
    int depth;
    int priority;
} CrawlURL;

void set_max_crawl_depth(int depth);
bool should_crawl_url(const char* url, int current_depth);
```

#### Benefits
- Controlled crawling scope
- Better resource management
- Focus on important content
- Avoid infinite crawling

---

### 3.8 Domain Whitelist/Blacklist üü° MEDIUM PRIORITY
**Time: 1 hour**

#### Purpose
Control which domains to crawl.

#### Features
- [ ] Whitelist: Only crawl these domains
- [ ] Blacklist: Never crawl these domains
- [ ] Pattern matching: `*.example.com`
- [ ] Configuration file support

#### Implementation
```c
typedef struct {
    char** whitelist;
    int whitelist_count;
    char** blacklist;
    int blacklist_count;
} DomainFilter;

bool is_domain_allowed(const char* domain);
void load_domain_filters(const char* config_file);
```

#### Benefits
- Focused crawling
- Avoid problematic sites
- Legal compliance
- Resource efficiency

---

### 3.9 Sitemap.xml Support üü¢ LOW PRIORITY
**Time: 2 hours**

#### Purpose
Discover URLs efficiently.

#### Features
- [ ] Download sitemap.xml
- [ ] Parse XML structure
- [ ] Extract all URLs
- [ ] Respect priority and changefreq
- [ ] Handle sitemap index files

#### Benefits
- Comprehensive URL discovery
- Efficient crawling
- Respect site structure
- Find all pages quickly

---

### 3.10 RSS/Atom Feed Support üü¢ LOW PRIORITY
**Time: 2 hours**

#### Purpose
Track new content automatically.

#### Features
- [ ] Detect feed URLs
- [ ] Parse RSS/Atom XML
- [ ] Extract article URLs
- [ ] Track last update time
- [ ] Periodic feed checking

#### Benefits
- Automatic content discovery
- Stay up-to-date
- Efficient monitoring
- Structured content

---

### 3.11 Crawl Statistics & Monitoring üü¢ LOW PRIORITY
**Time: 2 hours**

#### Purpose
Track crawler performance.

#### Metrics
- [ ] Pages crawled per hour
- [ ] Success/failure rates
- [ ] Average page size
- [ ] Processing time per page
- [ ] Queue size over time
- [ ] Domain distribution
- [ ] Error types and frequencies

#### Visualization
- [ ] Real-time dashboard
- [ ] Historical graphs
- [ ] Performance trends
- [ ] Bottleneck identification

---

### 3.12 Crystalline-Specific: Prime-Based Crawl Scheduling üîµ RESEARCH
**Time: 3 hours**

#### Purpose
Use crystalline mathematics for crawl timing.

#### Features
- [ ] Use prime numbers for delay calculation
- [ ] Map domains to prime numbers
- [ ] Use clock lattice for scheduling
- [ ] 12-fold symmetry in domain distribution

#### Benefits
- Mathematically elegant
- Unpredictable patterns
- Better distribution
- Aligns with crystalline design

---

### 3.13 Crystalline-Specific: Semantic URL Prioritization üîµ RESEARCH
**Time: 4 hours**

#### Purpose
Prioritize URLs based on semantic value.

#### Features
- [ ] Analyze URL structure
- [ ] Detect content type from URL
- [ ] Use prime encoding for priority
- [ ] Integrate with clock lattice mapping

#### Priority Factors
- URL depth
- Content type indicators
- Domain authority
- Semantic keywords

---

### 3.14 Crystalline-Specific: Crystalline Content Hashing üîµ RESEARCH
**Time: 2 hours**

#### Purpose
Use prime-based hashing for deduplication.

#### Features
- [ ] Hash content using prime numbers
- [ ] Map to clock lattice positions
- [ ] Use GCD for similarity detection
- [ ] 12-fold symmetry in hash space

#### Benefits
- Faster similarity detection
- Better distribution
- Aligns with crystalline math
- Unique to this system

---

## PHASE 4: COMPREHENSIVE UI REORGANIZATION WITH LEFT SIDEBAR (10 hours)

**STATUS: 40% COMPLETE - Phases 4.1, 4.2, 4.3 DONE**

### 4.1 Left Sidebar Implementation ‚úÖ COMPLETE (2 hours)
- [x] Created `app/ui/left_sidebar.c` and `app/ui/left_sidebar.h`
- [x] Implemented vertical tab navigation (200px width)
- [x] 5 main tab categories with emoji icons:
  * üé® Visualization (Prime Spiral, Calculator, Spheres, Folding, Video)
  * ü§ñ AI (LLM, Training, Research, Crawler)
  * üìä Models (Model Management)
  * ‚öôÔ∏è System (Benchmark)
  * üìÅ Data (URL Manager, Downloaded Files)
- [x] Implemented horizontal submenu system (40px height)
- [x] Context-sensitive submenus based on main tab
- [x] Click detection and event handling
- [x] Active tab highlighting with proper colors
- [x] Build verified: Zero errors, zero warnings

### 4.2 Main Render Loop Integration ‚úÖ COMPLETE (1 hour)
- [x] Added `#include "ui/left_sidebar.h"` to `app/main.c`
- [x] Created `sync_hierarchical_to_legacy_tab()` helper function
- [x] Updated `render()` function to call sidebar and submenu rendering
- [x] Updated `handle_mouse_click()` event handling
- [x] Initialized hierarchical tab system in `init_app()`
- [x] Build verified: Zero errors, zero warnings

### 4.3 Content Area Positioning ‚úÖ COMPLETE (1 hour)
- [x] Updated `app/ui/layout_manager.c` for sidebar offset
- [x] Changed tab_bar_height from 60 to 40 (submenu height)
- [x] All content now starts at (200, 40) - after sidebar and submenu
- [x] All layout calculations adjusted for new dimensions
- [x] Build verified: Zero errors, zero warnings

### 4.4 Data Tab Implementation ‚è≥ IN PROGRESS (6 hours)

**Purpose:** Implement the Data main tab with URL Manager and Downloaded Files viewer

#### 4.4.1 URL Manager Tab UI (3 hours)
- [ ] Create `app/ui/tabs/tab_url_manager.c` and `.h`
- [ ] Implement URL list view with sortable columns
- [ ] Implement filtering controls
- [ ] Implement action buttons
- [ ] Implement file type filter panel
- [ ] Implement block patterns panel
- [ ] Integrate with backend systems

#### 4.4.2 Downloaded Files Viewer Tab (3 hours)
- [ ] Create `app/ui/tabs/tab_downloaded_files.c` and `.h`
- [ ] Implement file browser with tree view
- [ ] Implement file list view with columns
- [ ] Implement file preview panel
- [ ] Implement search functionality
- [ ] Implement file actions
- [ ] Integrate with file database

### 4.5 Tab Layout Improvements (Optional - After 4.4)

**Note**: Basic 3-column layout completed in 4.0.3, this adds remaining features

#### Additional Features
- [ ] **Prime Configuration Panel**
  - Frequency prime input (uint64_t)
  - Link selection prime input
  - Delay min/max prime inputs
  - Enable/disable prime randomization toggle
  - Prime validation indicator

- [ ] **Link Management Panel**
  - Add link input field
  - Add button
  - Link list display (scrollable)
  - Remove link button
  - Clear all button
  - Queue size indicator

- [ ] **URL Pattern Selection**
  - Checkboxes for each pattern type
  - Enable/disable all button
  - Pattern statistics

- [ ] **Status Display**
  - Current crawl frequency
  - Next crawl time (prime-based)
  - Links in queue
  - Links crawled
  - Current URL being processed

#### Implementation
```c
// In app/ui/tabs/tab_crawler.c (new file)
void draw_crawler_tab(AppState* state);
void handle_crawler_tab_click(AppState* state, int x, int y);
void update_crawler_status(AppState* state);
```

### 4.2 Training Tab Advanced Features (After 4.0) (2 hours)

#### Features from Original Plan
- [ ] Recursive sub-spheres visualization
- [ ] Zoom and pan controls
- [ ] Mouse-over tooltips
- [ ] 3D visualization mode
- [ ] 2D/3D toggle

#### Implementation
```c
// Enhanced sphere visualization
typedef struct {
    int zoom_level;
    float pan_x, pan_y;
    bool show_3d;
    bool show_subspheres;
    int max_depth;
} SphereVisualizationConfig;
```

### 4.3 Collapsible Panels (After 4.0) (1 hour)

#### Complete Implementation
- [ ] Integrate collapsible headers into all tabs
- [ ] Add click detection
- [ ] Implement smooth animations
- [ ] Save panel states
- [ ] Keyboard shortcuts (Ctrl+number)

### 4.4 LLM Tab Remaining Features (After 4.0) (1 hour)

#### Optional Enhancements
- [ ] Repetition penalty slider
- [ ] Frequency penalty slider
- [ ] Presence penalty slider
- [ ] Stop sequences input
- [ ] Random seed input
- [ ] Cancel generation button
- [ ] Keyboard shortcuts (Ctrl+Enter to send)
- [ ] Context menu (right-click options)
- [ ] Drag and drop file upload

---

## PHASE 5: DEPENDENCY MANAGEMENT (2 hours)

### 5.1 Create Installation Scripts (1 hour)

#### Ubuntu/Debian Script
```bash
#!/bin/bash
# install_deps_ubuntu.sh

# System packages
apt-get update
apt-get install -y \
    build-essential \
    libcurl4-openssl-dev \
    libxml2-dev \
    libzip-dev \
    zlib1g-dev \
    poppler-utils \
    antiword \
    unrtf \
    tesseract-ocr \
    jq

# Optional: Build from source if needed
# ...
```

#### CentOS/RHEL Script
```bash
#!/bin/bash
# install_deps_centos.sh

# System packages
yum groupinstall -y "Development Tools"
yum install -y \
    libcurl-devel \
    libxml2-devel \
    libzip-devel \
    zlib-devel \
    poppler-utils \
    antiword \
    tesseract \
    jq

# Additional packages from EPEL
yum install -y epel-release
yum install -y unrtf
```

### 5.2 Dependency Documentation (1 hour)

#### Create DEPENDENCIES.md
- [ ] List all required libraries
- [ ] Version requirements
- [ ] Installation instructions per distro
- [ ] Build from source instructions
- [ ] Troubleshooting guide

---

## PHASE 6: TESTING & INTEGRATION (4 hours)

### 6.1 Unit Tests (2 hours)
- [ ] Test each file processor
- [ ] Test prime randomization
- [ ] Test URL pattern detection
- [ ] Test link management
- [ ] Test UI components

### 6.2 Integration Tests (2 hours)
- [ ] Test full crawler pipeline
- [ ] Test UI interactions
- [ ] Test with real-world data
- [ ] Performance benchmarking
- [ ] Memory leak detection

---

## PHASE 7: DOCUMENTATION (2 hours)

### 7.1 User Documentation
- [ ] Crawler configuration guide
- [ ] Prime randomization explanation
- [ ] URL pattern guide
- [ ] File format support matrix
- [ ] UI usage guide

### 7.2 Developer Documentation
- [ ] Library API reference
- [ ] Architecture diagrams
- [ ] Extension guide
- [ ] Contributing guidelines

---

## IMPLEMENTATION TIMELINE

### Total Estimated Time: ~34 hours

**Week 1** (16 hours)
- Phase 1: Library Reorganization (4h)
- Phase 2: Pure C File Processors (8h)
- Phase 3: Advanced Crawler Features (4h)

**Week 2** (18 hours)
- Phase 3: Advanced Crawler Features (2h remaining)
- Phase 4: Complete UI Enhancements (8h)
- Phase 5: Dependency Management (2h)
- Phase 6: Testing & Integration (4h)
- Phase 7: Documentation (2h)

---

## PRIORITY ORDER

### Critical Path (Must Complete First)
1. Library reorganization
2. Pure C file processors (DOCX/XLSX/PPTX)
3. Crawler prime randomization
4. Crawler UI enhancements

### Secondary Priority
5. Advanced URL patterns
6. Training tab enhancements
7. LLM tab remaining features
8. Collapsible panels

### Final Polish
9. Testing and integration
10. Documentation
11. Dependency scripts

---

## SUCCESS CRITERIA

### Phase Completion Checklist
- [ ] All Python code removed and replaced with C
- [ ] libdocproc library created and functional
- [ ] All 25+ file formats supported in pure C
- [ ] Prime-based randomization working
- [ ] Advanced URL pattern detection implemented
- [ ] Dynamic link management functional
- [ ] All UI enhancements complete
- [ ] Installation scripts for Ubuntu and CentOS
- [ ] Comprehensive documentation
- [ ] All tests passing
- [ ] No memory leaks
- [ ] Build successful on both Ubuntu and CentOS

---

## CURRENT STATUS

**Phase**: Planning Complete
**Next Step**: Begin Phase 1 - Library Reorganization
**Estimated Start**: Awaiting user confirmation

---

## NOTES

### Design Decisions
1. **Pure C**: No Python dependencies for production use
2. **Modular**: Each processor is independent
3. **Extensible**: Easy to add new formats
4. **Performant**: C implementations are faster than Python
5. **Portable**: Works on Ubuntu and CentOS

### External Dependencies (Minimal)
- libxml2 (XML parsing)
- libzip (ZIP handling)
- libcurl (HTTP - already used)
- poppler-utils (PDF - CLI tool)
- tesseract (OCR - CLI tool)

### Custom Implementations (No External Deps)
- YAML parser
- TOML parser
- CSV parser
- JSON parser (optional, can use jq)
- Office XML parsers
- ODF parsers
- EPUB parser

---

## QUESTIONS FOR USER

Before proceeding, please confirm:
1. ‚úÖ Remove all Python code and implement in pure C?
2. ‚úÖ Create unified libdocproc library?
3. ‚úÖ Implement prime-based randomization?
4. ‚úÖ Add all UI enhancements?
5. ‚úÖ Support both Ubuntu and CentOS?
6. ‚úÖ Custom C implementations where needed?

**Ready to begin implementation upon your approval!**
---

## üî¥ CRITICAL PRIORITY: GLOBAL MODEL MANAGEMENT ARCHITECTURE

### Problem Statement

**CRITICAL ARCHITECTURAL ISSUES IDENTIFIED:**

1. **Geometric Algorithm in Wrong Layer** ‚ùå
   - Currently in CLLM layer (`src/ai/cllm_lattice_embeddings_geometric.c`)
   - Should be in Algorithms layer (fundamental algorithm)
   - Violates layer separation principles

2. **No Global Model Manager** ‚ùå
   - Each tab creates independent models
   - Training tab has its own model
   - LLM tab has its own model
   - Crawler has its own model
   - CLI tools create separate models
   - Massive duplication and waste

3. **Models Recreated Every Time** ‚ùå
   - Opening a tab recreates the model
   - Slow initialization
   - Memory waste
   - Loss of training progress
   - No persistence

4. **No Concurrent Access** ‚ùå
   - Can't train and infer simultaneously
   - Training locks everything
   - Should support read-only inference during training

5. **No Model Selection UI** ‚ùå
   - No way to choose between models
   - No export/import functionality
   - No model management interface

### Correct Architecture

#### Layer 1: Algorithms Library (Fundamental)
```
algorithms/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ lattice_embeddings.c          # Geometric pattern algorithm
‚îî‚îÄ‚îÄ include/
    ‚îî‚îÄ‚îÄ lattice_embeddings.h          # Public API
```

**Purpose:** Pure geometric algorithm for ANY system to use

**API:**
```c
void lattice_embeddings_init_geometric(
    float* embeddings,
    uint32_t vocab_size,
    uint32_t embedding_dim
);
```

#### Layer 2: CLLM Library (Model Management)
```
src/ai/
‚îú‚îÄ‚îÄ cllm_model_manager.c              # Global model manager
‚îî‚îÄ‚îÄ cllm_model_registry.c             # Model registry/catalog

include/
‚îú‚îÄ‚îÄ cllm_model_manager.h              # Model manager API
‚îî‚îÄ‚îÄ cllm_model_registry.h             # Registry API
```

**Purpose:** Global model management for entire system

**Features:**
- Create/load/save models
- Model registry (catalog of available models)
- Concurrent access (read-only for inference)
- Model locking (exclusive for training)
- Model metadata (name, size, training status)

**API:**
```c
// Model Manager
typedef struct {
    char name[256];
    char path[1024];
    CLLMModel* model;
    bool is_training;
    bool is_loaded;
    pthread_rwlock_t lock;
} ManagedModel;

// Create/Load
ManagedModel* model_manager_create(const char* name, CLLMConfig* config);
ManagedModel* model_manager_load(const char* path);

// Access Control
CLLMModel* model_manager_acquire_read(const char* name);   // For inference
CLLMModel* model_manager_acquire_write(const char* name);  // For training
void model_manager_release(const char* name);

// Registry
ModelInfo* model_registry_list();
ModelInfo* model_registry_get(const char* name);
void model_registry_add(const char* name, const char* path);
void model_registry_remove(const char* name);
```

#### Layer 3: Application (UI Integration)
```
app/
‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îî‚îÄ‚îÄ tabs/
‚îÇ       ‚îú‚îÄ‚îÄ tab_models.c              # NEW: Model Management Tab
‚îÇ       ‚îú‚îÄ‚îÄ tab_training.c            # Uses model manager
‚îÇ       ‚îú‚îÄ‚îÄ tab_llm.c                 # Uses model manager
‚îÇ       ‚îî‚îÄ‚îÄ tab_crawler.c             # Uses model manager
‚îî‚îÄ‚îÄ model_selector.c                  # Model selection widget
```

**Purpose:** UI for model management

**Features:**
- List all available models
- Create new models
- Load existing models
- Delete models
- Export/import models
- Select model for each tab
- Show model status (training/idle/in-use)

#### Layer 4: CLI Tools (Shared Access)
```
tools/
‚îú‚îÄ‚îÄ cllm_train                        # Uses model manager
‚îú‚îÄ‚îÄ cllm_inference                    # Uses model manager
‚îî‚îÄ‚îÄ cllm_model_manager                # CLI for model management
```

**Purpose:** CLI access to same models as UI

**Features:**
- Same model manager API
- Can train while UI does inference
- Can use models created in UI
- Can create models for UI to use

### Implementation Plan

#### Phase 1: Move Geometric Algorithm to Algorithms Layer (HIGHEST PRIORITY)
- [ ] Move `src/ai/cllm_lattice_embeddings_geometric.c` ‚Üí `algorithms/src/lattice_embeddings.c`
- [ ] Move `include/cllm_lattice_embeddings.h` ‚Üí `algorithms/include/lattice_embeddings.h`
- [ ] Update `src/ai/cllm_create.c` to use algorithms layer
- [ ] Update `algorithms/Makefile` to include new file
- [ ] Update main `Makefile` linking
- [ ] Verify build with zero errors/warnings
- [ ] Test that embeddings still work correctly

#### Phase 2: Create Model Manager in CLLM Library (HIGH PRIORITY)
- [ ] Create `src/ai/cllm_model_manager.c`
  - [ ] Implement model creation
  - [ ] Implement model loading
  - [ ] Implement model saving
  - [ ] Implement concurrent access (pthread_rwlock_t)
  - [ ] Implement model locking
  - [ ] Implement model metadata
- [ ] Create `include/cllm_model_manager.h`
  - [ ] Define ManagedModel struct
  - [ ] Define API functions
  - [ ] Document usage
- [ ] Create `src/ai/cllm_model_registry.c`
  - [ ] Implement model catalog
  - [ ] Implement model search
  - [ ] Implement model persistence
- [ ] Create `include/cllm_model_registry.h`
  - [ ] Define ModelInfo struct
  - [ ] Define registry API
- [ ] Add to libcllm build
- [ ] Create unit tests

#### Phase 3: Create Model Management Tab (MEDIUM PRIORITY)
- [ ] Create `app/ui/tabs/tab_models.c`
  - [ ] Implement model list view
  - [ ] Implement create model dialog
  - [ ] Implement load model dialog
  - [ ] Implement delete model confirmation
  - [ ] Implement export model dialog
  - [ ] Implement import model dialog
  - [ ] Show model status (training/idle/in-use)
  - [ ] Show model metadata (size, vocab, layers)
- [ ] Create `app/model_selector.c`
  - [ ] Dropdown widget for model selection
  - [ ] Refresh button
  - [ ] Create new button
  - [ ] Load button
- [ ] Add tab to main UI
- [ ] Test UI functionality

#### Phase 4: Update Existing Tabs (MEDIUM PRIORITY)
- [ ] Update `app/ui/tabs/tab_training.c`
  - [ ] Remove local model creation
  - [ ] Add model selector widget
  - [ ] Use `model_manager_acquire_write()` for training
  - [ ] Release model when done
  - [ ] Handle model locking errors
- [ ] Update `app/ui/tabs/tab_llm.c`
  - [ ] Remove local model creation
  - [ ] Add model selector widget
  - [ ] Use `model_manager_acquire_read()` for inference
  - [ ] Release model when done
  - [ ] Support concurrent inference
- [ ] Update `app/ui/tabs/tab_crawler.c`
  - [ ] Remove local model creation
  - [ ] Add model selector widget
  - [ ] Use appropriate access mode
  - [ ] Release model when done
- [ ] Test tab integration

#### Phase 5: Update CLI Tools (LOW PRIORITY)
- [ ] Update `tools/cllm_train`
  - [ ] Use model manager API
  - [ ] Support model selection by name
  - [ ] Support creating new models
- [ ] Update `tools/cllm_inference`
  - [ ] Use model manager API
  - [ ] Support model selection by name
  - [ ] Support read-only access
- [ ] Create `tools/cllm_model_manager`
  - [ ] List models
  - [ ] Create models
  - [ ] Delete models
  - [ ] Export models
  - [ ] Import models
  - [ ] Show model info
- [ ] Test CLI integration

#### Phase 6: Testing and Validation (LOW PRIORITY)
- [ ] Test concurrent access
  - [ ] Train in one tab, infer in another
  - [ ] Multiple inference sessions
  - [ ] Proper locking behavior
- [ ] Test model persistence
  - [ ] Save and load models
  - [ ] Resume training
  - [ ] Model metadata preserved
- [ ] Test UI integration
  - [ ] Model selection works
  - [ ] Status updates correctly
  - [ ] Error handling
- [ ] Test CLI integration
  - [ ] CLI can use UI-created models
  - [ ] UI can use CLI-created models
  - [ ] Concurrent CLI and UI access
- [ ] Test cross-tool access
  - [ ] Train in CLI, infer in UI
  - [ ] Train in UI, infer in CLI
  - [ ] Multiple tools simultaneously

### Benefits

#### 1. Efficiency
- Models created once, reused everywhere
- No recreation overhead
- Shared memory across tabs and tools

#### 2. Consistency
- Same model across all tabs
- Same model between UI and CLI
- Single source of truth

#### 3. Flexibility
- Choose different models for different tasks
- Train one model while using another
- Multiple models available simultaneously

#### 4. Concurrency
- Read-only inference during training
- Multiple inference sessions
- Proper locking prevents corruption

#### 5. Persistence
- Models saved automatically
- Resume training from checkpoints
- Export/import for sharing

### Success Criteria

- [ ] Geometric algorithm in algorithms layer
- [ ] Model manager in CLLM library
- [ ] Model management tab in UI
- [ ] All tabs use model manager
- [ ] All CLI tools use model manager
- [ ] Can train and infer simultaneously
- [ ] Models persist across sessions
- [ ] Zero build errors/warnings
- [ ] All tests pass

### Priority Order

1. **HIGHEST:** Move geometric algorithm to algorithms layer
2. **HIGH:** Create model manager in CLLM library
3. **MEDIUM:** Create model management tab
4. **MEDIUM:** Update existing tabs
5. **LOW:** Update CLI tools
6. **LOW:** Testing and validation


---

## üÜï NEW OBJECTIVE: COMPREHENSIVE URL MANAGEMENT SYSTEM

### Priority: HIGH (User Request)

### Overview
Implement a complete URL management system with database backend, filtering, blocking, prioritization, and UI for managing crawled content.

### Requirements from User
1. Add and remove URLs easily
2. Preserve GET parameters in URLs (critical!)
3. Block specific pages/links/websites
4. File type selection/deselection
5. Block patterns (regex support)
6. Prioritize uncrawled pages (timestamp tracking)
7. View downloaded pages/files
8. Comprehensive URL manager with all features

### Phase 1: URL Database Backend

#### 1.1: Create URL Database Schema
**File:** `src/crawler/url_database.h`

```c
#ifndef URL_DATABASE_H
#define URL_DATABASE_H

#include <stdint.h>
#include <stdbool.h>
#include <time.h>

// URL entry in database
typedef struct {
    uint64_t id;
    char url[4096];              // Full URL with GET parameters
    char domain[256];
    char path[2048];
    char query_string[1024];     // GET parameters preserved
    char file_type[32];          // html, pdf, txt, etc.
    time_t first_seen;
    time_t last_crawled;
    int crawl_count;
    int priority;
    char status[32];             // pending, crawled, failed, blocked
    char source_url[4096];
    bool blocked;
} URLEntry;

// URL database handle
typedef struct URLDatabase URLDatabase;

// Create/open database
URLDatabase* url_db_open(const char* db_path);
void url_db_close(URLDatabase* db);

// URL operations
int url_db_add(URLDatabase* db, const char* url, const char* source_url);
int url_db_remove(URLDatabase* db, uint64_t id);
int url_db_block(URLDatabase* db, uint64_t id);
int url_db_unblock(URLDatabase* db, uint64_t id);
int url_db_mark_crawled(URLDatabase* db, uint64_t id);
int url_db_mark_failed(URLDatabase* db, uint64_t id);

// Query operations
URLEntry* url_db_get_next(URLDatabase* db);  // Get highest priority uncrawled
URLEntry** url_db_query(URLDatabase* db, const char* filter, int* count);
URLEntry* url_db_get_by_id(URLDatabase* db, uint64_t id);
bool url_db_exists(URLDatabase* db, const char* url);

// Statistics
int url_db_count_total(URLDatabase* db);
int url_db_count_pending(URLDatabase* db);
int url_db_count_crawled(URLDatabase* db);
int url_db_count_blocked(URLDatabase* db);

// Export/Import
int url_db_export(URLDatabase* db, const char* file_path);
int url_db_import(URLDatabase* db, const char* file_path);

#endif // URL_DATABASE_H
```

#### 1.2: Implement URL Database
**File:** `src/crawler/url_database.c`

Use SQLite for robust database management:
- Table: urls (id, url, domain, path, query_string, file_type, first_seen, last_crawled, crawl_count, priority, status, source_url, blocked)
- Indexes on: url (unique), domain, status, priority, last_crawled
- Preserve full URL including GET parameters
- Parse URL to extract domain, path, query string
- Calculate priority based on: uncrawled (high), domain diversity, timestamp

#### 1.3: Create URL Filter System
**File:** `src/crawler/url_filter.h`

```c
#ifndef URL_FILTER_H
#define URL_FILTER_H

#include <stdbool.h>

// Filter configuration
typedef struct {
    // File type filters
    bool allow_html;
    bool allow_pdf;
    bool allow_txt;
    bool allow_doc;
    bool allow_images;
    bool allow_video;
    bool allow_audio;
    bool allow_archives;
    bool allow_code;
    bool allow_data;
    
    // Domain filters
    char** domain_whitelist;
    int whitelist_count;
    char** domain_blacklist;
    int blacklist_count;
    
    // Pattern filters
    char** url_patterns;      // Regex patterns
    int pattern_count;
    
    // GET parameter handling
    bool preserve_query_params;
    bool strip_tracking_params;
    char** tracking_param_names;
    int tracking_param_count;
} URLFilterConfig;

// Filter handle
typedef struct URLFilter URLFilter;

// Create filter
URLFilter* url_filter_create(URLFilterConfig* config);
void url_filter_destroy(URLFilter* filter);

// Filter operations
bool url_filter_should_crawl(URLFilter* filter, const char* url);
bool url_filter_is_allowed_type(URLFilter* filter, const char* file_type);
bool url_filter_is_allowed_domain(URLFilter* filter, const char* domain);
bool url_filter_matches_pattern(URLFilter* filter, const char* url);

// Configuration
int url_filter_add_domain_whitelist(URLFilter* filter, const char* domain);
int url_filter_add_domain_blacklist(URLFilter* filter, const char* domain);
int url_filter_add_pattern(URLFilter* filter, const char* pattern);
int url_filter_remove_domain_whitelist(URLFilter* filter, const char* domain);
int url_filter_remove_domain_blacklist(URLFilter* filter, const char* domain);
int url_filter_remove_pattern(URLFilter* filter, const char* pattern);

// Save/Load
int url_filter_save(URLFilter* filter, const char* file_path);
int url_filter_load(URLFilter* filter, const char* file_path);

#endif // URL_FILTER_H
```

#### 1.4: Create URL Blocker System
**File:** `src/crawler/url_blocker.h`

```c
#ifndef URL_BLOCKER_H
#define URL_BLOCKER_H

#include <stdbool.h>

// Block pattern types
typedef enum {
    BLOCK_EXACT_URL,
    BLOCK_DOMAIN,
    BLOCK_PATH_PREFIX,
    BLOCK_REGEX_PATTERN
} BlockPatternType;

// Block pattern
typedef struct {
    BlockPatternType type;
    char pattern[2048];
    char description[256];
    time_t added_time;
} BlockPattern;

// Blocker handle
typedef struct URLBlocker URLBlocker;

// Create blocker
URLBlocker* url_blocker_create(const char* patterns_file);
void url_blocker_destroy(URLBlocker* blocker);

// Block operations
int url_blocker_add_pattern(URLBlocker* blocker, BlockPatternType type, const char* pattern, const char* description);
int url_blocker_remove_pattern(URLBlocker* blocker, int pattern_id);
bool url_blocker_is_blocked(URLBlocker* blocker, const char* url);

// Query patterns
BlockPattern** url_blocker_get_patterns(URLBlocker* blocker, int* count);
BlockPattern* url_blocker_get_pattern(URLBlocker* blocker, int pattern_id);

// Test pattern
bool url_blocker_test_pattern(URLBlocker* blocker, const char* pattern, const char* test_url);

// Save/Load
int url_blocker_save(URLBlocker* blocker);
int url_blocker_load(URLBlocker* blocker);

#endif // URL_BLOCKER_H
```

#### 1.5: Create URL Prioritization System
**File:** `src/crawler/url_priority.h`

```c
#ifndef URL_PRIORITY_H
#define URL_PRIORITY_H

#include "url_database.h"
#include <stdint.h>

// Priority factors
typedef struct {
    float uncrawled_bonus;      // Bonus for never-crawled URLs
    float domain_diversity;     // Bonus for underrepresented domains
    float time_decay;           // Decay for recently crawled
    float depth_penalty;        // Penalty for deep URLs
    float prime_randomization;  // Prime-based random factor
} PriorityFactors;

// Priority calculator
typedef struct URLPriority URLPriority;

// Create priority calculator
URLPriority* url_priority_create(PriorityFactors* factors);
void url_priority_destroy(URLPriority* priority);

// Calculate priority
int url_priority_calculate(URLPriority* priority, URLEntry* entry, int domain_count);

// Update domain statistics
void url_priority_update_domain_stats(URLPriority* priority, const char* domain);

// Get prime-based random value
uint64_t url_priority_prime_random(URLPriority* priority, uint64_t seed);

#endif // URL_PRIORITY_H
```

### Phase 2: Downloaded Files Management

#### 2.1: Create Downloaded Files Database
**File:** `src/crawler/file_database.h`

```c
#ifndef FILE_DATABASE_H
#define FILE_DATABASE_H

#include <stdint.h>
#include <time.h>

// Downloaded file entry
typedef struct {
    uint64_t id;
    char url[4096];
    char file_path[2048];
    char domain[256];
    char file_type[32];
    uint64_t file_size;
    time_t download_time;
    char content_type[128];
    int http_status;
} FileEntry;

// File database handle
typedef struct FileDatabase FileDatabase;

// Create/open database
FileDatabase* file_db_open(const char* db_path);
void file_db_close(FileDatabase* db);

// File operations
int file_db_add(FileDatabase* db, const char* url, const char* file_path);
int file_db_remove(FileDatabase* db, uint64_t id);
FileEntry* file_db_get_by_url(FileDatabase* db, const char* url);
FileEntry** file_db_query(FileDatabase* db, const char* filter, int* count);

// Statistics
int file_db_count_total(FileDatabase* db);
int file_db_count_by_domain(FileDatabase* db, const char* domain);
int file_db_count_by_type(FileDatabase* db, const char* file_type);
uint64_t file_db_total_size(FileDatabase* db);

#endif // FILE_DATABASE_H
```

---

## üÜï NEW OBJECTIVE: UI REORGANIZATION WITH LEFT SIDEBAR

### Priority: HIGH (User Request)

### Overview
Reorganize the entire UI to use a left sidebar with vertical tabs and hierarchical submenus.

### Requirements from User
1. Tabs on left side instead of top
2. Main AI tab containing: LLM, Training, Research, Crawler
3. Submenu system for organizing related tabs
4. Collapsible sections
5. Icons for each tab

### Phase 1: Update AppState Structure

#### 1.1: Update app/app_common.h

```c
// Main tab categories (left sidebar)
typedef enum {
    MAIN_TAB_AI,
    MAIN_TAB_MODELS,
    MAIN_TAB_SYSTEM,
    MAIN_TAB_DATA,
    MAIN_TAB_COUNT
} MainTab;

// AI sub-tabs
typedef enum {
    AI_SUB_LLM,
    AI_SUB_TRAINING,
    AI_SUB_RESEARCH,
    AI_SUB_CRAWLER,
    AI_SUB_COUNT
} AISubTab;

// Models sub-tabs
typedef enum {
    MODELS_SUB_MANAGEMENT,
    MODELS_SUB_COUNT
} ModelsSubTab;

// System sub-tabs
typedef enum {
    SYSTEM_SUB_BENCHMARK,
    SYSTEM_SUB_ADAPTERS,
    SYSTEM_SUB_COUNT
} SystemSubTab;

// Data sub-tabs
typedef enum {
    DATA_SUB_URL_MANAGER,
    DATA_SUB_DOWNLOADED_FILES,
    DATA_SUB_COUNT
} DataSubTab;

// Update AppState
typedef struct {
    // ... existing fields ...
    
    // New tab system
    MainTab main_tab;
    union {
        AISubTab ai_sub;
        ModelsSubTab models_sub;
        SystemSubTab system_sub;
        DataSubTab data_sub;
    } sub_tab;
    
    // ... rest of fields ...
} AppState;
```

### Phase 2: Implement Left Sidebar

#### 2.1: Create app/ui/left_sidebar.c

```c
#include "left_sidebar.h"
#include "../app_common.h"

#define SIDEBAR_WIDTH 200
#define TAB_HEIGHT 50
#define ICON_SIZE 24

// Render left sidebar with vertical tabs
void render_left_sidebar(SDL_Renderer* renderer, AppState* state) {
    // Background
    SDL_SetRenderDrawColor(renderer, 40, 40, 45, 255);
    SDL_Rect sidebar = {0, 0, SIDEBAR_WIDTH, WINDOW_HEIGHT};
    SDL_RenderFillRect(renderer, &sidebar);
    
    // Render main tabs
    int y = 10;
    
    // AI Tab
    render_main_tab(renderer, state, MAIN_TAB_AI, "ü§ñ AI", 0, y);
    y += TAB_HEIGHT;
    
    // Models Tab
    render_main_tab(renderer, state, MAIN_TAB_MODELS, "üìä Models", 0, y);
    y += TAB_HEIGHT;
    
    // System Tab
    render_main_tab(renderer, state, MAIN_TAB_SYSTEM, "‚öôÔ∏è System", 0, y);
    y += TAB_HEIGHT;
    
    // Data Tab
    render_main_tab(renderer, state, MAIN_TAB_DATA, "üìÅ Data", 0, y);
    y += TAB_HEIGHT;
}

// Render individual main tab button
void render_main_tab(SDL_Renderer* renderer, AppState* state, MainTab tab, const char* label, int x, int y) {
    SDL_Rect button = {x, y, SIDEBAR_WIDTH, TAB_HEIGHT};
    
    // Highlight if active
    if (state->main_tab == tab) {
        SDL_SetRenderDrawColor(renderer, 60, 60, 70, 255);
    } else {
        SDL_SetRenderDrawColor(renderer, 50, 50, 55, 255);
    }
    
    SDL_RenderFillRect(renderer, &button);
    
    // Border
    SDL_SetRenderDrawColor(renderer, 80, 80, 90, 255);
    SDL_RenderDrawRect(renderer, &button);
    
    // Label (render text)
    // ... text rendering code ...
}

// Handle sidebar click
void handle_sidebar_click(AppState* state, int x, int y) {
    if (x > SIDEBAR_WIDTH) return;
    
    int tab_index = y / TAB_HEIGHT;
    
    switch (tab_index) {
        case 0: state->main_tab = MAIN_TAB_AI; break;
        case 1: state->main_tab = MAIN_TAB_MODELS; break;
        case 2: state->main_tab = MAIN_TAB_SYSTEM; break;
        case 3: state->main_tab = MAIN_TAB_DATA; break;
    }
}
```

### Phase 3: Implement Submenu System

#### 3.1: Create app/ui/submenu.c

```c
#include "submenu.h"
#include "../app_common.h"

#define SUBMENU_HEIGHT 40
#define SUBMENU_Y (0)
#define SUBMENU_X (SIDEBAR_WIDTH)
#define SUBMENU_WIDTH (WINDOW_WIDTH - SIDEBAR_WIDTH)

// Render horizontal submenu bar
void render_submenu(SDL_Renderer* renderer, AppState* state) {
    // Background
    SDL_SetRenderDrawColor(renderer, 50, 50, 55, 255);
    SDL_Rect submenu_bar = {SUBMENU_X, SUBMENU_Y, SUBMENU_WIDTH, SUBMENU_HEIGHT};
    SDL_RenderFillRect(renderer, &submenu_bar);
    
    // Render submenu items based on main tab
    switch (state->main_tab) {
        case MAIN_TAB_AI:
            render_ai_submenu(renderer, state);
            break;
        case MAIN_TAB_MODELS:
            render_models_submenu(renderer, state);
            break;
        case MAIN_TAB_SYSTEM:
            render_system_submenu(renderer, state);
            break;
        case MAIN_TAB_DATA:
            render_data_submenu(renderer, state);
            break;
    }
}

// Render AI submenu
void render_ai_submenu(SDL_Renderer* renderer, AppState* state) {
    int x = SUBMENU_X + 10;
    int button_width = 120;
    
    render_submenu_button(renderer, state, "LLM Chat", x, SUBMENU_Y, button_width, SUBMENU_HEIGHT, 
                         state->sub_tab.ai_sub == AI_SUB_LLM);
    x += button_width + 10;
    
    render_submenu_button(renderer, state, "Training", x, SUBMENU_Y, button_width, SUBMENU_HEIGHT,
                         state->sub_tab.ai_sub == AI_SUB_TRAINING);
    x += button_width + 10;
    
    render_submenu_button(renderer, state, "Research", x, SUBMENU_Y, button_width, SUBMENU_HEIGHT,
                         state->sub_tab.ai_sub == AI_SUB_RESEARCH);
    x += button_width + 10;
    
    render_submenu_button(renderer, state, "Crawler", x, SUBMENU_Y, button_width, SUBMENU_HEIGHT,
                         state->sub_tab.ai_sub == AI_SUB_CRAWLER);
}

// Handle submenu click
void handle_submenu_click(AppState* state, int x, int y) {
    if (y > SUBMENU_HEIGHT) return;
    if (x < SUBMENU_X) return;
    
    int button_index = (x - SUBMENU_X - 10) / 130;  // 120 width + 10 spacing
    
    switch (state->main_tab) {
        case MAIN_TAB_AI:
            if (button_index >= 0 && button_index < AI_SUB_COUNT) {
                state->sub_tab.ai_sub = (AISubTab)button_index;
            }
            break;
        // ... other cases ...
    }
}
```

### Phase 4: Create URL Manager Tab

#### 4.1: Create app/ui/tabs/tab_url_manager.c

```c
#include "tab_url_manager.h"
#include "../../app_common.h"
#include "../../../src/crawler/url_database.h"
#include "../../../src/crawler/url_filter.h"
#include "../../../src/crawler/url_blocker.h"

typedef struct {
    URLDatabase* db;
    URLFilter* filter;
    URLBlocker* blocker;
    
    // UI state
    int selected_url_id;
    int scroll_offset;
    char search_query[256];
    char add_url_input[4096];
    char add_pattern_input[2048];
    
    // Filters
    bool show_pending;
    bool show_crawled;
    bool show_blocked;
    char domain_filter[256];
} URLManagerState;

// Initialize URL manager tab
void tab_url_manager_init(AppState* state) {
    URLManagerState* url_state = calloc(1, sizeof(URLManagerState));
    
    url_state->db = url_db_open("data/urls.db");
    url_state->filter = url_filter_create(NULL);  // Load from config
    url_state->blocker = url_blocker_create("data/block_patterns.txt");
    
    url_state->show_pending = true;
    url_state->show_crawled = true;
    url_state->show_blocked = false;
    
    state->url_manager_state = url_state;
}

// Render URL manager tab
void tab_url_manager_render(SDL_Renderer* renderer, AppState* state) {
    URLManagerState* url_state = state->url_manager_state;
    
    int content_x = SIDEBAR_WIDTH;
    int content_y = SUBMENU_HEIGHT;
    int content_w = WINDOW_WIDTH - SIDEBAR_WIDTH;
    int content_h = WINDOW_HEIGHT - SUBMENU_HEIGHT;
    
    // Split into 3 panels: URL list (left), actions (top-right), filters (bottom-right)
    int list_w = content_w * 0.6;
    int panel_w = content_w * 0.4;
    int actions_h = content_h * 0.4;
    int filters_h = content_h * 0.6;
    
    // Render URL list
    render_url_list(renderer, url_state, content_x, content_y, list_w, content_h);
    
    // Render actions panel
    render_actions_panel(renderer, url_state, content_x + list_w, content_y, panel_w, actions_h);
    
    // Render filters panel
    render_filters_panel(renderer, url_state, content_x + list_w, content_y + actions_h, panel_w, filters_h);
}

// Render URL list with sortable columns
void render_url_list(SDL_Renderer* renderer, URLManagerState* state, int x, int y, int w, int h) {
    // Header
    SDL_SetRenderDrawColor(renderer, 60, 60, 70, 255);
    SDL_Rect header = {x, y, w, 30};
    SDL_RenderFillRect(renderer, &header);
    
    // Columns: [Select] [URL] [Domain] [Status] [Last Crawled]
    int col_x = x + 5;
    render_text(renderer, "‚òê", col_x, y + 5);  // Checkbox
    col_x += 30;
    render_text(renderer, "URL", col_x, y + 5);
    col_x += w * 0.4;
    render_text(renderer, "Domain", col_x, y + 5);
    col_x += w * 0.2;
    render_text(renderer, "Status", col_x, y + 5);
    col_x += w * 0.2;
    render_text(renderer, "Last Crawled", col_x, y + 5);
    
    // URL entries
    int entry_y = y + 35;
    int entry_h = 25;
    
    // Query URLs from database
    int count = 0;
    URLEntry** entries = url_db_query(state->db, NULL, &count);
    
    for (int i = state->scroll_offset; i < count && entry_y < y + h; i++) {
        URLEntry* entry = entries[i];
        
        // Alternate row colors
        if (i % 2 == 0) {
            SDL_SetRenderDrawColor(renderer, 45, 45, 50, 255);
        } else {
            SDL_SetRenderDrawColor(renderer, 50, 50, 55, 255);
        }
        
        SDL_Rect row = {x, entry_y, w, entry_h};
        SDL_RenderFillRect(renderer, &row);
        
        // Render entry data
        col_x = x + 5;
        render_text(renderer, entry->id == state->selected_url_id ? "‚òë" : "‚òê", col_x, entry_y + 5);
        col_x += 30;
        render_text_truncated(renderer, entry->url, col_x, entry_y + 5, w * 0.4 - 10);
        col_x += w * 0.4;
        render_text(renderer, entry->domain, col_x, entry_y + 5);
        col_x += w * 0.2;
        render_text(renderer, entry->status, col_x, entry_y + 5);
        col_x += w * 0.2;
        
        char time_str[64];
        format_time(entry->last_crawled, time_str, sizeof(time_str));
        render_text(renderer, time_str, col_x, entry_y + 5);
        
        entry_y += entry_h;
    }
    
    // Free query results
    free(entries);
}

// Render actions panel
void render_actions_panel(SDL_Renderer* renderer, URLManagerState* state, int x, int y, int w, int h) {
    // Background
    SDL_SetRenderDrawColor(renderer, 55, 55, 60, 255);
    SDL_Rect panel = {x, y, w, h};
    SDL_RenderFillRect(renderer, &panel);
    
    // Title
    render_text(renderer, "Actions", x + 10, y + 10);
    
    int button_y = y + 40;
    int button_h = 30;
    int button_w = w - 20;
    
    // Add URL input and button
    render_text(renderer, "Add URL:", x + 10, button_y);
    button_y += 20;
    render_input_field(renderer, state->add_url_input, x + 10, button_y, button_w, 25);
    button_y += 30;
    render_button(renderer, "Add", x + 10, button_y, button_w, button_h);
    button_y += button_h + 10;
    
    // Remove selected button
    render_button(renderer, "Remove Selected", x + 10, button_y, button_w, button_h);
    button_y += button_h + 10;
    
    // Block selected button
    render_button(renderer, "Block Selected", x + 10, button_y, button_w, button_h);
    button_y += button_h + 10;
    
    // Export button
    render_button(renderer, "Export List", x + 10, button_y, button_w, button_h);
    button_y += button_h + 10;
    
    // Import button
    render_button(renderer, "Import List", x + 10, button_y, button_w, button_h);
}

// Render filters panel
void render_filters_panel(SDL_Renderer* renderer, URLManagerState* state, int x, int y, int w, int h) {
    // Background
    SDL_SetRenderDrawColor(renderer, 55, 55, 60, 255);
    SDL_Rect panel = {x, y, w, h};
    SDL_RenderFillRect(renderer, &panel);
    
    // Title
    render_text(renderer, "Filters", x + 10, y + 10);
    
    int checkbox_y = y + 40;
    int checkbox_h = 25;
    
    // Status filters
    render_checkbox(renderer, "Show Pending", state->show_pending, x + 10, checkbox_y);
    checkbox_y += checkbox_h;
    render_checkbox(renderer, "Show Crawled", state->show_crawled, x + 10, checkbox_y);
    checkbox_y += checkbox_h;
    render_checkbox(renderer, "Show Blocked", state->show_blocked, x + 10, checkbox_y);
    checkbox_y += checkbox_h + 10;
    
    // File type filters
    render_text(renderer, "File Types:", x + 10, checkbox_y);
    checkbox_y += 20;
    render_checkbox(renderer, "HTML", state->filter->allow_html, x + 10, checkbox_y);
    checkbox_y += checkbox_h;
    render_checkbox(renderer, "PDF", state->filter->allow_pdf, x + 10, checkbox_y);
    checkbox_y += checkbox_h;
    render_checkbox(renderer, "TXT", state->filter->allow_txt, x + 10, checkbox_y);
    checkbox_y += checkbox_h;
    render_checkbox(renderer, "Images", state->filter->allow_images, x + 10, checkbox_y);
    checkbox_y += checkbox_h + 10;
    
    // Block patterns
    render_text(renderer, "Block Patterns:", x + 10, checkbox_y);
    checkbox_y += 20;
    render_input_field(renderer, state->add_pattern_input, x + 10, checkbox_y, w - 20, 25);
    checkbox_y += 30;
    render_button(renderer, "Add Pattern", x + 10, checkbox_y, w - 20, 30);
}
```

### Phase 5: Create Downloaded Files Viewer Tab

#### 5.1: Create app/ui/tabs/tab_downloaded_files.c

Similar structure to URL manager, but for viewing downloaded files:
- File browser with tree view
- File preview panel
- File actions (open, delete, re-crawl)
- Search functionality

---

## Implementation Priority

1. **Phase 1.2-1.5**: Implement URL database, filter, blocker, priority systems (Backend)
2. **Phase 2**: Implement downloaded files database (Backend)
3. **Phase 3**: Update UI structure (app_common.h, left sidebar, submenu)
4. **Phase 4**: Implement URL Manager tab (UI)
5. **Phase 5**: Implement Downloaded Files Viewer tab (UI)
6. **Phase 6**: Integration with existing crawler
7. **Phase 7**: Testing and validation


---

## üî¥ CRITICAL: COMPREHENSIVE UI/UX FIXES (NEW - HIGHEST PRIORITY)

### Date Added: 2024-11-30
### Status: CRITICAL - MUST FIX IMMEDIATELY

Based on comprehensive user feedback, the following critical issues have been identified:

---

### PHASE 1: CRITICAL UI FIXES

#### 1.1 Connect Crawler Tab to URL Manager Backend

**Problem:**
- Crawler tab uses old in-memory system
- Does NOT use SQLite database
- Does NOT use crawler_url_manager.c backend
- URLs persist incorrectly from previous sessions

**Solution:**
```c
// In app/ui/tabs/tab_crawler.c
#include "../../../src/crawler/crawler_url_manager.h"

// Replace all in-memory URL handling with:
CrawlerURLManager* url_manager = crawler_url_manager_create("crawler.db");

// Add URL:
crawler_url_manager_add(url_manager, url_string);

// Get next URL:
char* next_url = crawler_url_manager_get_next(url_manager);

// Mark crawled:
crawler_url_manager_mark_crawled(url_manager, url_id);
```

**Files to Modify:**
- `app/ui/tabs/tab_crawler.c` - Add includes, replace URL handling
- `app/crawler_thread.c` - Use URL manager for crawling

**Testing:**
- [ ] URLs persist correctly across sessions
- [ ] URLs don't duplicate from previous sessions
- [ ] Crawler and Data tab show same URLs

---

#### 1.2 Fix Crawler Tab Layout Issues

**Problems:**
1. Buttons too wide (extend past window edge)
2. "Web Crawler Control Center" text overlaps top tabs
3. Prime configuration inputs have no labels
4. URL patterns unclear if clickable

**Solutions:**

**1. Reduce Button Widths:**
```c
// Current (WRONG):
int button_width = 200;  // Too wide

// Fixed (CORRECT):
int button_width = 150;  // Fits in panel
```

**2. Move Overlapping Text:**
```c
// Current (WRONG):
draw_text("Web Crawler Control Center", x, 10);  // Overlaps tabs

// Fixed (CORRECT):
draw_text("Web Crawler Control Center", x, 60);  // Below tabs
```

**3. Add Input Labels:**
```c
// Add labels for each prime configuration input:
draw_text("Randomization Seed:", x, y);
draw_text("Prime Modulus:", x, y + 40);
draw_text("Selection Method:", x, y + 80);
```

**4. Make URL Patterns Clickable:**
```c
// Add checkbox rendering:
draw_checkbox(x, y, pattern->enabled);
draw_text(pattern->name, x + 30, y);
```

**Files to Modify:**
- `app/ui/tabs/tab_crawler.c` - Fix all layout issues

---

#### 1.3 Connect Models Tab to Model Manager

**Problem:**
- Shows "no models available"
- Doesn't call `model_manager_list()`
- Inconsistent with LLM/Training tabs

**Solution:**
```c
// In app/ui/tabs/tab_models.c

// Get list of models:
char** model_names = NULL;
size_t model_count = 0;
model_manager_list(&model_names, &model_count);

// Display each model:
for (size_t i = 0; i < model_count; i++) {
    CLLMModelStatus status;
    model_manager_get_status(model_names[i], &status);
    
    // Draw model entry with status
    draw_model_entry(x, y, model_names[i], &status);
    y += 40;
}
```

**Files to Modify:**
- `app/ui/tabs/tab_models.c` - Add model_manager integration

---

#### 1.4 Fix Visualization Centering

**Problem:**
- All visualizations not centered
- Calculator not centered
- Layout doesn't account for 200px sidebar

**Solution:**
```c
// In app/ui/layout_manager.c

// Current (WRONG):
int content_x = 0;
int content_width = WINDOW_WIDTH;

// Fixed (CORRECT):
int content_x = SIDEBAR_WIDTH;  // 200px
int content_width = WINDOW_WIDTH - SIDEBAR_WIDTH - CONTROL_PANEL_WIDTH;

// Center calculation:
int center_x = content_x + (content_width / 2);
int center_y = SUBMENU_HEIGHT + ((WINDOW_HEIGHT - SUBMENU_HEIGHT) / 2);
```

**Files to Modify:**
- `app/ui/layout_manager.c` - Fix centering calculations
- `app/visualization.c` - Use corrected center coordinates
- `app/calculator.c` - Use corrected center coordinates

---

### PHASE 2: HIGH PRIORITY FIXES

#### 2.1 Connect Data Tab to SQLite Database

**Problem:**
- Only shows "demo" data
- Not connected to url_database
- Not showing actual crawled URLs

**Solution:**
```c
// In app/ui/tabs/tab_downloaded_files.c

#include "../../../src/crawler/url_database.h"

// Get crawled URLs:
URLDatabase* db = url_database_open("crawler.db");
URLEntry* urls = url_database_get_crawled(db, &count);

// Display each URL:
for (size_t i = 0; i < count; i++) {
    draw_url_entry(x, y, &urls[i]);
    y += 30;
}
```

**Files to Modify:**
- `app/ui/tabs/tab_downloaded_files.c` - Connect to database

---

#### 2.2 Fix Video Tab Issues

**Problems:**
1. Uses Unicode characters (violates standards)
2. Overlapping text and text boxes

**Solutions:**

**1. Replace Unicode with ASCII:**
```c
// Current (WRONG):
draw_text("‚ñ∂ Play", x, y);  // Unicode

// Fixed (CORRECT):
draw_text("> Play", x, y);  // ASCII
```

**2. Fix Overlapping Layout:**
```c
// Add proper spacing:
int spacing = 10;
int y_offset = 0;

draw_text("Label:", x, y + y_offset);
y_offset += 30;  // Space before text box

draw_textbox(x, y + y_offset, width, height);
y_offset += height + spacing;  // Space after text box
```

**Files to Modify:**
- `app/ui/tabs/tab_video.c` - Replace unicode, fix layout

---

### PHASE 3: NEW CRAWLER FEATURES

#### 3.1 Human Text Extraction (vs Metadata)

**Requirement:**
Extract only human-generated text, ignore HTML metadata, scripts, styles, etc.

**Implementation:**

**Backend:**
```c
// Create src/crawler/text_extractor.c

typedef struct {
    bool extract_human_text_only;
    bool remove_scripts;
    bool remove_styles;
    bool remove_navigation;
    bool remove_ads;
} TextExtractionOptions;

char* extract_human_text(const char* html, TextExtractionOptions* opts);
```

**UI Integration:**
```c
// In app/ui/tabs/tab_crawler.c
draw_checkbox(x, y, "Extract human text only");
draw_checkbox(x, y + 30, "Remove scripts/styles");
draw_checkbox(x, y + 60, "Remove navigation");
```

**CLI Integration:**
```bash
./cllm_crawler --human-text-only --remove-scripts
```

**Files to Create:**
- `src/crawler/text_extractor.c`
- `src/crawler/text_extractor.h`

**Files to Modify:**
- `app/ui/tabs/tab_crawler.c` - Add checkboxes
- `tools/cllm_crawler.c` - Add CLI flags

---

#### 3.2 X.com (Twitter) Profile Crawler

**Requirement:**
Crawl X.com user profiles without API, extract posts and comments.

**Implementation:**

**Backend:**
```c
// Create src/crawler/xcom_crawler.c

typedef struct {
    char* username;
    bool include_replies;
    bool include_retweets;
    int max_posts;
} XComCrawlerOptions;

typedef struct {
    char* text;
    char* timestamp;
    int likes;
    int retweets;
    bool is_reply;
} XComPost;

XComPost* crawl_xcom_profile(const char* username, XComCrawlerOptions* opts, size_t* count);
```

**UI Integration:**
```c
// In app/ui/tabs/tab_crawler.c
draw_checkbox(x, y, "X.com profile mode");
draw_textbox(x, y + 30, "Username:");
draw_checkbox(x, y + 70, "Include replies");
draw_checkbox(x, y + 100, "Include retweets");
```

**CLI Integration:**
```bash
./cllm_crawler --xcom-profile username --include-replies --max-posts 1000
```

**Files to Create:**
- `src/crawler/xcom_crawler.c`
- `src/crawler/xcom_crawler.h`

**Files to Modify:**
- `app/ui/tabs/tab_crawler.c` - Add X.com options
- `tools/cllm_crawler.c` - Add CLI flags

---

#### 3.3 Britannica/Etymonline Crawlers

**Requirement:**
Site-specific crawlers for Britannica.com and Etymonline.com

**Implementation:**

**Backend:**
```c
// Create src/crawler/site_specific_crawlers.c

typedef enum {
    SITE_BRITANNICA,
    SITE_ETYMONLINE,
    SITE_GENERIC
} SiteType;

typedef struct {
    char* title;
    char* content;
    char* url;
    SiteType site_type;
} ArticleContent;

SiteType detect_site_type(const char* url);
ArticleContent* extract_article_content(const char* html, SiteType site_type);
```

**UI Integration:**
```c
// In app/ui/tabs/tab_crawler.c
draw_checkbox(x, y, "Britannica mode");
draw_checkbox(x, y + 30, "Etymonline mode");
draw_checkbox(x, y + 60, "Auto-detect site type");
```

**CLI Integration:**
```bash
./cllm_crawler --site-type britannica --url "https://www.britannica.com/..."
./cllm_crawler --site-type etymonline --url "https://www.etymonline.com/..."
```

**Files to Create:**
- `src/crawler/site_specific_crawlers.c`
- `src/crawler/site_specific_crawlers.h`

**Files to Modify:**
- `app/ui/tabs/tab_crawler.c` - Add site-specific options
- `tools/cllm_crawler.c` - Add CLI flags

---

#### 3.4 GET Parameter Handling

**Requirement:**
Proper awareness and handling of GET parameters in URLs

**Current State:**
- `url_database.c` already preserves query_string ‚úÖ
- Need to verify and add UI display

**Implementation:**

**Verify Backend:**
```c
// In src/crawler/url_database.c
// Verify query_string field is populated:
url->query_string = extract_query_string(url_string);
```

**UI Integration:**
```c
// In app/ui/tabs/tab_url_manager.c
// Display GET parameters:
if (url->query_string && strlen(url->query_string) > 0) {
    draw_text("GET params:", x, y);
    draw_text(url->query_string, x + 100, y);
}
```

**Files to Verify:**
- `src/crawler/url_database.c` - Verify query_string handling

**Files to Modify:**
- `app/ui/tabs/tab_url_manager.c` - Display GET parameters

---

### PHASE 4: SITE ANALYSIS

#### 4.1 Analyze Target Sites

**Sites to Analyze:**
1. X.com (Twitter) profile pages
2. Britannica.com article pages
3. Etymonline.com word pages

**Analysis Tasks:**
- [ ] Examine HTML structure of each site
- [ ] Identify human text vs metadata
- [ ] Identify navigation elements
- [ ] Identify ad elements
- [ ] Identify main content containers
- [ ] Document CSS selectors for content extraction

**Tools to Use:**
- `web-search` to find example pages
- `scrape-webpage` to get HTML
- `browser_navigate_to` for dynamic content

---

### PHASE 5: PREPROCESSOR UPDATES

#### 5.1 Enhance Preprocessor

**Current State:**
- Basic HTML to text conversion
- No selective extraction

**Required Enhancements:**
- [ ] Add human text extraction
- [ ] Add metadata removal
- [ ] Add script/style removal
- [ ] Add navigation removal
- [ ] Add site-specific extraction
- [ ] Add GET parameter preservation

**Files to Modify:**
- `src/crawler/preprocessor.c` - Add new extraction modes

---

### PHASE 6: CLI TOOL UPDATES

#### 6.1 Add New CLI Options

**Tool: cllm_crawler**

**New Options:**
```bash
--human-text-only          Extract only human text
--remove-scripts           Remove scripts and styles
--remove-navigation        Remove navigation elements
--xcom-profile USERNAME    Crawl X.com profile
--include-replies          Include replies (X.com)
--include-retweets         Include retweets (X.com)
--site-type TYPE           Use site-specific parser (britannica|etymonline)
--max-posts N              Maximum posts to crawl
```

**Files to Modify:**
- `tools/cllm_crawler.c` - Add all new options

---

### IMPLEMENTATION PRIORITY

**Immediate (This Session):**
1. Fix Crawler Tab Layout (1.2)
2. Connect Crawler to URL Manager (1.1)
3. Fix Visualization Centering (1.4)
4. Connect Models Tab (1.3)

**Next Session:**
5. Connect Data Tab (2.1)
6. Fix Video Tab (2.2)
7. Implement Human Text Extraction (3.1)

**Future Sessions:**
8. Implement X.com Crawler (3.2)
9. Implement Site-Specific Crawlers (3.3)
10. Analyze Target Sites (4.1)

---

**END OF NEW REQUIREMENTS**

---

## üî¥ HIGH PRIORITY: LIBRARY STRUCTURE REORGANIZATION

### Problem
Currently, all libraries share the same directory structure with mixed includes and sources. This creates:
- Cluttered directory structure
- Difficult dependency management
- Hard to maintain independent libraries
- Prevents creating separate repositories for each library

### Goal
Reorganize each library into its own independent directory with proper `src/` and `include/` structure, similar to how `algorithms/` is already organized.

### Current Structure (PROBLEMATIC)
```
crystalline/
‚îú‚îÄ‚îÄ include/          # Mixed headers for ALL libraries
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ geometry/
‚îÇ   ‚îú‚îÄ‚îÄ ai/
‚îÇ   ‚îú‚îÄ‚îÄ crawler/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ src/              # Mixed sources for ALL libraries
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ geometry/
‚îÇ   ‚îú‚îÄ‚îÄ ai/
‚îÇ   ‚îú‚îÄ‚îÄ crawler/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ algorithms/       # ‚úÖ Already properly structured
    ‚îú‚îÄ‚îÄ include/
    ‚îî‚îÄ‚îÄ src/
```

### Target Structure (CLEAN)
```
crystalline/
‚îú‚îÄ‚îÄ crystalline/      # Core crystalline math library
‚îÇ   ‚îú‚îÄ‚îÄ include/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ geometry/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transcendental/
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îú‚îÄ‚îÄ core/
‚îÇ       ‚îú‚îÄ‚îÄ geometry/
‚îÇ       ‚îî‚îÄ‚îÄ transcendental/
‚îÇ
‚îú‚îÄ‚îÄ algorithms/       # ‚úÖ Already correct
‚îÇ   ‚îú‚îÄ‚îÄ include/
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ
‚îú‚îÄ‚îÄ cllm/             # CLLM AI library
‚îÇ   ‚îú‚îÄ‚îÄ include/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ai/
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îú‚îÄ‚îÄ ai/
‚îÇ       ‚îî‚îÄ‚îÄ infrastructure/
‚îÇ
‚îú‚îÄ‚îÄ crawler/          # Web crawler library
‚îÇ   ‚îú‚îÄ‚îÄ include/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ crawler/
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îî‚îÄ‚îÄ crawler/
‚îÇ
‚îú‚îÄ‚îÄ docproc/          # Document processing library
‚îÇ   ‚îú‚îÄ‚îÄ include/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ docproc/
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îî‚îÄ‚îÄ docproc/
‚îÇ
‚îú‚îÄ‚îÄ app/              # Application (uses all libraries)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ tools/            # CLI tools (use all libraries)
    ‚îî‚îÄ‚îÄ ...
```

### Benefits
1. **Independent Management**: Each library can be developed independently
2. **Clear Dependencies**: Library dependencies are explicit
3. **Separate Repositories**: Easy to split into separate repos later
4. **Clean Includes**: `#include <crystalline/core/bigint.h>` instead of `#include "../../../include/core/bigint.h"`
5. **Better Build System**: Each library has its own Makefile
6. **Easier Testing**: Each library can be tested independently

### Implementation Plan

#### Phase 1: Create New Directory Structure
- [ ] Create `crystalline/` directory with `include/` and `src/`
- [ ] Create `cllm/` directory with `include/` and `src/`
- [ ] Create `crawler/` directory with `include/` and `src/`
- [ ] Create `docproc/` directory with `include/` and `src/`

#### Phase 2: Move Files
- [ ] Move core, geometry, transcendental to `crystalline/`
- [ ] Move ai and infrastructure to `cllm/`
- [ ] Move crawler files to `crawler/`
- [ ] Move document processing to `docproc/`

#### Phase 3: Update Include Paths
- [ ] Update all `#include` statements in source files
- [ ] Update all `#include` statements in header files
- [ ] Update Makefiles with new paths
- [ ] Update `-I` flags in compiler commands

#### Phase 4: Update Build System
- [ ] Create Makefile for each library
- [ ] Update root Makefile to build all libraries
- [ ] Update library linking order
- [ ] Test build system

#### Phase 5: Verify and Test
- [ ] Verify all libraries build independently
- [ ] Verify all tools build correctly
- [ ] Verify application builds correctly
- [ ] Run tests to ensure functionality

### Priority
**HIGH PRIORITY** - This should be done before creating separate repositories and will make future maintenance much easier.

### Estimated Time
- Phase 1: 30 minutes
- Phase 2: 1-2 hours
- Phase 3: 2-3 hours (many files to update)
- Phase 4: 1-2 hours
- Phase 5: 1 hour
**Total: 5-8 hours**

### Notes
- This is a large refactoring but necessary for long-term maintainability
- Should be done in a separate branch and thoroughly tested
- Will make the codebase much cleaner and easier to understand
- Essential for eventually creating separate repositories

---


## CRITICAL ISSUES - IMMEDIATE FIXES REQUIRED

### OBJECTIVE: Fix Memory Corruption (Core Dump)
**Priority:** CRITICAL
**Status:** NOT STARTED

#### Problem
Application crashes with "malloc(): smallbin double linked list corrupted" when switching tabs.

#### Root Cause
Memory corruption from:
- Double-free errors
- Buffer overflows
- Use-after-free
- Invalid pointer manipulation

#### Tasks
- [ ] Run application with valgrind to detect memory errors
- [ ] Enable Address Sanitizer (ASAN) for debugging
- [ ] Check all malloc/free pairs for double-free
- [ ] Verify all buffer sizes in string operations
- [ ] Check pointer assignments in ButtonManager
- [ ] Check pointer assignments in InputManager
- [ ] Fix all detected memory errors
- [ ] Test tab switching thoroughly

#### Estimated Time
2-4 hours

---

### OBJECTIVE: Fix Input System Failures
**Priority:** CRITICAL
**Status:** NOT STARTED

#### Problem
Inputs not working across multiple tabs:
- URL Manager tab: `url_manager.add_url` input not responding
- Crawler tab: `crawler.add_url` input not responding
- User can click inputs (they get focus) but cannot type text

#### Root Cause
InputManager handles focus but text input is not being processed or displayed correctly.

#### Tasks
- [ ] Verify SDL_StartTextInput() is called when input is focused
- [ ] Check if text is being stored in ManagedInput structure
- [ ] Verify rendering code displays input text correctly
- [ ] Check if SDL_TEXTINPUT events are being processed
- [ ] Test text input on all tabs
- [ ] Verify cursor rendering
- [ ] Test backspace and delete keys
- [ ] Test Enter key submission

#### Estimated Time
1-2 hours

---

### OBJECTIVE: Complete ButtonManager System
**Priority:** HIGH
**Status:** 80% COMPLETE (BUILD BROKEN)

#### Problem
ButtonManager system incomplete:
- 13 undefined callback functions
- Build fails with linker errors
- Application cannot compile

#### Missing Callbacks
1. Crawler: `crawler_add_url_clicked`, `crawler_start_clicked`, `crawler_stop_clicked`
2. Training: `training_start_clicked`, `training_stop_clicked`, `training_load_data_clicked`
3. Models: `models_create_clicked`, `models_delete_clicked`, `models_load_clicked`
4. URL Manager: `url_manager_add_clicked`, `url_manager_remove_clicked`, `url_manager_block_clicked`, `url_manager_refresh_clicked`

#### Tasks
- [ ] Create stub implementations for all 13 callbacks
- [ ] Wire callbacks to existing tab functionality
- [ ] Test each button individually
- [ ] Remove duplicate button handling from tab click handlers
- [ ] Integration test across all tabs
- [ ] Verify no memory leaks in ButtonManager
- [ ] Document ButtonManager architecture

#### Estimated Time
2-3 hours

---

### OBJECTIVE: Integrate Crawler with URL Database
**Priority:** HIGH
**Status:** NOT STARTED

#### Problem
Crawler ignores URL database and uses hardcoded URLs:
- User adds URLs to database via UI ‚úì
- Database stores URLs correctly ‚úì
- Crawler does NOT read from database ‚úó
- Crawler uses hardcoded "https://www.britannica.com/" ‚úó

#### Root Cause
Crawler is not integrated with CrawlerURLManager database system.

#### Tasks
- [ ] Find where crawler gets start URL (likely crawler_thread.c)
- [ ] Replace hardcoded URL with database query
- [ ] Implement url_db_get_next_url() integration
- [ ] Remove all hardcoded URLs from crawler
- [ ] Test URL addition via UI
- [ ] Test crawler using database URLs
- [ ] Verify URL status updates in database
- [ ] Test URL prioritization

#### Estimated Time
1-2 hours

---

### OBJECTIVE: Fix Model Initialization
**Priority:** MEDIUM
**Status:** NOT STARTED

#### Problem
Crawler forces creation of new model despite existing model:
- Application loads "saved_model" at startup ‚úì
- Crawler looks for model named "model" ‚úó
- Model not found, creates new 50000-token model ‚úó
- Wastes time re-initializing primes, kissing spheres, embeddings ‚úó

#### Root Cause
Hardcoded model name mismatch in crawler code.

#### Tasks
- [ ] Find where crawler requests model by name
- [ ] Change to use existing model ("saved_model")
- [ ] OR: Make model name configurable
- [ ] OR: Use first available model if specific name not found
- [ ] Test crawler with existing model
- [ ] Verify no unnecessary model initialization
- [ ] Document model naming convention

#### Estimated Time
30 minutes - 1 hour

---

### OBJECTIVE: Revert ButtonManager Changes (CONTINGENCY)
**Priority:** CONTINGENCY
**Status:** NOT STARTED

#### When to Use
If fixing ButtonManager takes too long or causes more issues.

#### Tasks
- [ ] Identify last working commit (before ButtonManager)
- [ ] Create backup branch of current work
- [ ] Revert to last working commit
- [ ] Verify application compiles
- [ ] Verify application runs without crashes
- [ ] Test all basic functionality
- [ ] Document what was reverted

#### Estimated Time
30 minutes

---
