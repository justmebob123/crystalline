/**
 * Integration Test: Forward and Backward Pass
 * 
 * Tests that forward and backward passes work together correctly
 */

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>
#include "../../include/cllm.h"
#include "../../include/cllm_training.h"

// Helper: Create a minimal test model
CLLMModel* create_minimal_model() {
    CLLMModel* model = (CLLMModel*)calloc(1, sizeof(CLLMModel));
    if (!model) return NULL;
    
    // Minimal model configuration
    model->vocab_size = 50;
    model->embedding_dim = 64;
    model->num_layers = 1;
    
    // Allocate embeddings
    model->embeddings.vocab_size = model->vocab_size;
    model->embeddings.embedding_dim = model->embedding_dim;
    model->embeddings.embeddings = (float*)calloc(
        model->vocab_size * model->embedding_dim, sizeof(float));
    
    // Initialize embeddings with small random values
    for (size_t i = 0; i < model->vocab_size * model->embedding_dim; i++) {
        model->embeddings.embeddings[i] = ((float)rand() / RAND_MAX - 0.5f) * 0.1f;
    }
    
    // Allocate attention layers
    model->attention_layers = (AttentionLayer*)calloc(model->num_layers, sizeof(AttentionLayer));
    model->attention_layers[0].num_heads = 2;
    model->attention_layers[0].head_dim = 32;
    
    uint32_t dim = model->attention_layers[0].num_heads * model->attention_layers[0].head_dim;
    size_t weight_size = dim * dim;
    
    model->attention_layers[0].query_lattice = (float*)calloc(weight_size, sizeof(float));
    model->attention_layers[0].key_lattice = (float*)calloc(weight_size, sizeof(float));
    model->attention_layers[0].value_lattice = (float*)calloc(weight_size, sizeof(float));
    
    // Initialize with small random values
    for (size_t j = 0; j < weight_size; j++) {
        model->attention_layers[0].query_lattice[j] = ((float)rand() / RAND_MAX - 0.5f) * 0.1f;
        model->attention_layers[0].key_lattice[j] = ((float)rand() / RAND_MAX - 0.5f) * 0.1f;
        model->attention_layers[0].value_lattice[j] = ((float)rand() / RAND_MAX - 0.5f) * 0.1f;
    }
    
    // Allocate feedforward layers
    model->ff_layers = (FeedForwardLayer*)calloc(model->num_layers, sizeof(FeedForwardLayer));
    model->ff_layers[0].input_dim = model->embedding_dim;
    model->ff_layers[0].hidden_dim = 256;
    model->ff_layers[0].output_dim = model->embedding_dim;
    
    size_t w1_size = model->ff_layers[0].input_dim * model->ff_layers[0].hidden_dim;
    size_t w2_size = model->ff_layers[0].hidden_dim * model->ff_layers[0].output_dim;
    
    model->ff_layers[0].w1_lattice = (float*)calloc(w1_size, sizeof(float));
    model->ff_layers[0].w2_lattice = (float*)calloc(w2_size, sizeof(float));
    model->ff_layers[0].bias1 = (float*)calloc(model->ff_layers[0].hidden_dim, sizeof(float));
    model->ff_layers[0].bias2 = (float*)calloc(model->ff_layers[0].output_dim, sizeof(float));
    
    // Initialize with small random values
    for (size_t j = 0; j < w1_size; j++) {
        model->ff_layers[0].w1_lattice[j] = ((float)rand() / RAND_MAX - 0.5f) * 0.1f;
    }
    for (size_t j = 0; j < w2_size; j++) {
        model->ff_layers[0].w2_lattice[j] = ((float)rand() / RAND_MAX - 0.5f) * 0.1f;
    }
    
    // Allocate layer norms
    model->layer_norms = (CLLMLayerNorm*)calloc(model->num_layers, sizeof(CLLMLayerNorm));
    model->layer_norms[0].dim = model->embedding_dim;
    model->layer_norms[0].gamma = (float*)calloc(model->embedding_dim, sizeof(float));
    model->layer_norms[0].beta = (float*)calloc(model->embedding_dim, sizeof(float));
    
    // Initialize gamma to 1, beta to 0
    for (uint32_t j = 0; j < model->embedding_dim; j++) {
        model->layer_norms[0].gamma[j] = 1.0f;
        model->layer_norms[0].beta[j] = 0.0f;
    }
    
    return model;
}

// Helper: Free minimal model
void free_minimal_model(CLLMModel* model) {
    if (!model) return;
    
    free(model->embeddings.embeddings);
    
    if (model->attention_layers) {
        free(model->attention_layers[0].query_lattice);
        free(model->attention_layers[0].key_lattice);
        free(model->attention_layers[0].value_lattice);
        free(model->attention_layers);
    }
    
    if (model->ff_layers) {
        free(model->ff_layers[0].w1_lattice);
        free(model->ff_layers[0].w2_lattice);
        free(model->ff_layers[0].bias1);
        free(model->ff_layers[0].bias2);
        free(model->ff_layers);
    }
    
    if (model->layer_norms) {
        free(model->layer_norms[0].gamma);
        free(model->layer_norms[0].beta);
        free(model->layer_norms);
    }
    
    free(model);
}

// Test 1: Training initialization with attention cache
int test_training_init_with_cache() {
    printf("Test 1: Training initialization with attention cache... ");
    
    CLLMModel* model = create_minimal_model();
    if (!model) {
        printf("FAIL (model creation failed)\n");
        return 0;
    }
    
    CLLMTrainingConfig config = {
        .learning_rate = 0.001f,
        .batch_size = 4,
        .sequence_length = 16,
        .num_epochs = 1,
        .max_steps = 100,
        .warmup_steps = 10,
        .optimizer = "adam",
        .lr_scheduler = "cosine"
    };
    
    CLLMTraining* training = cllm_training_init(model, &config);
    
    int success = 0;
    if (training) {
        // Check that attention cache is allocated
        if (training->attention_cache && training->store_attention_weights) {
            // Check that cache buffers are allocated
            if (training->attention_cache[0].queries &&
                training->attention_cache[0].keys &&
                training->attention_cache[0].values &&
                training->attention_cache[0].attention_weights &&
                training->attention_cache[0].scores) {
                success = 1;
            }
        }
        cllm_training_cleanup(training);
    }
    
    free_minimal_model(model);
    
    if (success) {
        printf("PASS\n");
        return 1;
    } else {
        printf("FAIL (cache not properly allocated)\n");
        return 0;
    }
}

// Test 2: Attention cache populated during forward pass
int test_cache_population() {
    printf("Test 2: Attention cache populated during forward pass... ");
    
    CLLMModel* model = create_minimal_model();
    if (!model) {
        printf("FAIL (model creation failed)\n");
        return 0;
    }
    
    CLLMTrainingConfig config = {
        .learning_rate = 0.001f,
        .batch_size = 2,
        .sequence_length = 8,
        .num_epochs = 1,
        .max_steps = 100,
        .warmup_steps = 10,
        .optimizer = "adam",
        .lr_scheduler = "cosine"
    };
    
    CLLMTraining* training = cllm_training_init(model, &config);
    
    int success = 0;
    if (training) {
        // Create dummy input tokens
        uint32_t* input_tokens = (uint32_t*)calloc(
            config.batch_size * config.sequence_length, sizeof(uint32_t));
        
        // Fill with valid token IDs
        for (int i = 0; i < config.batch_size * config.sequence_length; i++) {
            input_tokens[i] = i % model->vocab_size;
        }
        
        // Note: We can't easily test the full forward pass here without
        // exposing internal functions, but we can verify the cache structure
        // is ready to be populated
        
        // Check cache is ready
        if (training->attention_cache && training->store_attention_weights) {
            success = 1;
        }
        
        free(input_tokens);
        cllm_training_cleanup(training);
    }
    
    free_minimal_model(model);
    
    if (success) {
        printf("PASS\n");
        return 1;
    } else {
        printf("FAIL (cache not ready)\n");
        return 0;
    }
}

// Test 3: Gradient buffers allocated
int test_gradient_buffers() {
    printf("Test 3: Gradient buffers allocated... ");
    
    CLLMModel* model = create_minimal_model();
    if (!model) {
        printf("FAIL (model creation failed)\n");
        return 0;
    }
    
    CLLMTrainingConfig config = {
        .learning_rate = 0.001f,
        .batch_size = 2,
        .sequence_length = 8,
        .num_epochs = 1,
        .max_steps = 100,
        .warmup_steps = 10,
        .optimizer = "adam",
        .lr_scheduler = "cosine"
    };
    
    CLLMTraining* training = cllm_training_init(model, &config);
    
    int success = 0;
    if (training) {
        // Check gradient buffers
        if (training->attention_grads &&
            training->attention_grads[0].query_lattice &&
            training->attention_grads[0].key_lattice &&
            training->attention_grads[0].value_lattice) {
            success = 1;
        }
        cllm_training_cleanup(training);
    }
    
    free_minimal_model(model);
    
    if (success) {
        printf("PASS\n");
        return 1;
    } else {
        printf("FAIL (gradient buffers not allocated)\n");
        return 0;
    }
}

// Test 4: Feature flag control
int test_feature_flag_control() {
    printf("Test 4: Feature flag control... ");
    
    CLLMModel* model = create_minimal_model();
    if (!model) {
        printf("FAIL (model creation failed)\n");
        return 0;
    }
    
    CLLMTrainingConfig config = {
        .learning_rate = 0.001f,
        .batch_size = 2,
        .sequence_length = 8,
        .num_epochs = 1,
        .max_steps = 100,
        .warmup_steps = 10,
        .optimizer = "adam",
        .lr_scheduler = "cosine"
    };
    
    CLLMTraining* training = cllm_training_init(model, &config);
    
    int success = 0;
    if (training) {
        // Check default state (should be enabled)
        int default_enabled = training->store_attention_weights;
        
        // Try disabling
        training->store_attention_weights = 0;
        int can_disable = (training->store_attention_weights == 0);
        
        // Try re-enabling
        training->store_attention_weights = 1;
        int can_enable = (training->store_attention_weights == 1);
        
        success = default_enabled && can_disable && can_enable;
        
        cllm_training_cleanup(training);
    }
    
    free_minimal_model(model);
    
    if (success) {
        printf("PASS\n");
        return 1;
    } else {
        printf("FAIL (feature flag not working)\n");
        return 0;
    }
}

// Test 5: Memory consistency check
int test_memory_consistency() {
    printf("Test 5: Memory consistency check... ");
    
    CLLMModel* model = create_minimal_model();
    if (!model) {
        printf("FAIL (model creation failed)\n");
        return 0;
    }
    
    CLLMTrainingConfig config = {
        .learning_rate = 0.001f,
        .batch_size = 2,
        .sequence_length = 8,
        .num_epochs = 1,
        .max_steps = 100,
        .warmup_steps = 10,
        .optimizer = "adam",
        .lr_scheduler = "cosine"
    };
    
    // Multiple allocation/deallocation cycles
    int success = 1;
    for (int cycle = 0; cycle < 5; cycle++) {
        CLLMTraining* training = cllm_training_init(model, &config);
        
        if (!training || !training->attention_cache) {
            success = 0;
            if (training) cllm_training_cleanup(training);
            break;
        }
        
        cllm_training_cleanup(training);
    }
    
    free_minimal_model(model);
    
    if (success) {
        printf("PASS (5 cycles)\n");
        return 1;
    } else {
        printf("FAIL (memory consistency issue)\n");
        return 0;
    }
}

int main() {
    printf("\n");
    printf("╔══════════════════════════════════════════════════════════╗\n");
    printf("║     Forward/Backward Integration Tests                  ║\n");
    printf("╚══════════════════════════════════════════════════════════╝\n");
    printf("\n");
    
    srand(42);  // Fixed seed for reproducibility
    
    int passed = 0;
    int total = 5;
    
    passed += test_training_init_with_cache();
    passed += test_cache_population();
    passed += test_gradient_buffers();
    passed += test_feature_flag_control();
    passed += test_memory_consistency();
    
    printf("\n");
    printf("═══════════════════════════════════════════════════════════\n");
    printf("Results: %d/%d tests passed (%.1f%%)\n", passed, total, 
           (float)passed / total * 100.0f);
    printf("═══════════════════════════════════════════════════════════\n");
    printf("\n");
    
    return (passed == total) ? 0 : 1;
}