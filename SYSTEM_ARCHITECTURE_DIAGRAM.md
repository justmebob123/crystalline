# System Architecture Diagrams
## Crystalline CLLM Visual Reference

---

## 1. LIBRARY DEPENDENCY HIERARCHY

```
┌─────────────────────────────────────────────────────────────────────┐
│                        APPLICATION LAYER                             │
│                                                                      │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐             │
│  │              │  │              │  │              │             │
│  │  CLI Tools   │  │   UI App     │  │    Demos     │             │
│  │              │  │              │  │              │             │
│  │ train_model  │  │ training_    │  │ cllm_demo    │             │
│  │ cllm_infer   │  │   thread     │  │ pretrain_    │             │
│  │ cllm_tokenize│  │ ui_main      │  │   model      │             │
│  │              │  │              │  │              │             │
│  └──────┬───────┘  └──────┬───────┘  └──────┬───────┘             │
│         │                 │                 │                      │
│         └─────────────────┼─────────────────┘                      │
│                           │                                        │
└───────────────────────────┼────────────────────────────────────────┘
                            │
                            │ API Calls
                            ↓
┌───────────────────────────┼────────────────────────────────────────┐
│                      CLLM LIBRARY LAYER                             │
│                                                                      │
│  ┌──────────────────────────────────────────────────────────────┐  │
│  │                     libcllm.so                                │  │
│  │                                                               │  │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐         │  │
│  │  │             │  │             │  │             │         │  │
│  │  │  Training   │  │  Inference  │  │ Tokenizer   │         │  │
│  │  │             │  │             │  │             │         │  │
│  │  │ • Forward   │  │ • Generate  │  │ • Encode    │         │  │
│  │  │ • Backward  │  │ • Sample    │  │ • Decode    │         │  │
│  │  │ • Optimizer │  │ • Beam      │  │ • Vocab     │         │  │
│  │  │ • Loss      │  │   Search    │  │   Build     │         │  │
│  │  │             │  │             │  │             │         │  │
│  │  └─────────────┘  └─────────────┘  └─────────────┘         │  │
│  │                                                               │  │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐         │  │
│  │  │             │  │             │  │             │         │  │
│  │  │   Model     │  │  Attention  │  │ Feed-Forward│         │  │
│  │  │             │  │             │  │             │         │  │
│  │  │ • Create    │  │ • Multi-Head│  │ • Linear    │         │  │
│  │  │ • Load      │  │ • Scaled    │  │ • ReLU      │         │  │
│  │  │ • Save      │  │   Dot Prod  │  │ • GELU      │         │  │
│  │  │ • Format    │  │ • Softmax   │  │             │         │  │
│  │  │             │  │             │  │             │         │  │
│  │  └─────────────┘  └─────────────┘  └─────────────┘         │  │
│  │                                                               │  │
│  └───────────────────────┬───────────────────────────────────────┘  │
│                          │                                          │
│                          │ Uses prime_* functions                   │
│                          ↓                                          │
└───────────────────────────┼──────────────────────────────────────────┘
                            │
┌───────────────────────────┼──────────────────────────────────────────┐
│                 CRYSTALLINE LATTICE LAYER                            │
│                                                                      │
│  ┌──────────────────────────────────────────────────────────────┐  │
│  │                  libcrystalline.so                            │  │
│  │                                                               │  │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐         │  │
│  │  │             │  │             │  │             │         │  │
│  │  │ Prime Math  │  │   Lattice   │  │   Abacus    │         │  │
│  │  │             │  │             │  │             │         │  │
│  │  │ • sqrtf     │  │ • L_lattice │  │ • create    │         │  │
│  │  │ • expf      │  │ • theta_n   │  │ • next_prime│         │  │
│  │  │ • logf      │  │ • r_n       │  │ • is_prime  │         │  │
│  │  │ • sinf      │  │ • Z_n_d     │  │ • free      │         │  │
│  │  │ • cosf      │  │ • P_n_d_k   │  │             │         │  │
│  │  │ • tanhf     │  │             │  │             │         │  │
│  │  │ • powf      │  │             │  │             │         │  │
│  │  │             │  │             │  │             │         │  │
│  │  └─────────────┘  └─────────────┘  └─────────────┘         │  │
│  │                                                               │  │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐         │  │
│  │  │             │  │             │  │             │         │  │
│  │  │   BigInt    │  │  BigFixed   │  │   Matrix    │         │  │
│  │  │             │  │             │  │             │         │  │
│  │  │ • Arbitrary │  │ • Fixed-    │  │ • Operations│         │  │
│  │  │   Precision │  │   Point     │  │ • SIMD      │         │  │
│  │  │ • Add/Sub   │  │ • Scale     │  │ • Cache     │         │  │
│  │  │ • Mul/Div   │  │ • Convert   │  │   Optimized │         │  │
│  │  │             │  │             │  │             │         │  │
│  │  └─────────────┘  └─────────────┘  └─────────────┘         │  │
│  │                                                               │  │
│  └───────────────────────────────────────────────────────────────┘  │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────────────────┐
│                      AUXILIARY LIBRARIES                              │
│                                                                      │
│  ┌──────────────────────┐  ┌──────────────────────┐                │
│  │   libcrawler.so      │  │   libdocproc.so      │                │
│  │                      │  │                      │                │
│  │ • Web Crawling       │  │ • PDF Processing     │                │
│  │ • Data Collection    │  │ • OCR                │                │
│  │ • Continuous Training│  │ • Document Extract   │                │
│  │                      │  │                      │                │
│  └──────────────────────┘  └──────────────────────┘                │
│                                                                      │
└──────────────────────────────────────────────────────────────────────┘
```

---

## 2. TRAINING PIPELINE DATA FLOW

```
┌─────────────────────────────────────────────────────────────────────┐
│                         INPUT STAGE                                  │
└─────────────────────────────────────────────────────────────────────┘
                            │
                            ↓
                    ┌───────────────┐
                    │  Text Files   │
                    │               │
                    │ • .txt        │
                    │ • .md         │
                    │ • .json       │
                    └───────┬───────┘
                            │
                            ↓ cllm_load_training_data()
                    ┌───────────────┐
                    │  Tokenization │
                    │               │
                    │ • Vocab-based │
                    │ • Char-based  │
                    │ • APPEND mode │
                    └───────┬───────┘
                            │
                            ↓
                    ┌───────────────┐
                    │  Token Array  │
                    │               │
                    │ [t₁, t₂, ...] │
                    └───────┬───────┘
                            │
┌───────────────────────────┼─────────────────────────────────────────┐
│                      BATCH PREPARATION                               │
└───────────────────────────┼─────────────────────────────────────────┘
                            │
                            ↓ cllm_get_batch()
                    ┌───────────────┐
                    │ Batch Extract │
                    │               │
                    │ Input:  [B×S] │
                    │ Target: [B×S] │
                    └───────┬───────┘
                            │
┌───────────────────────────┼─────────────────────────────────────────┐
│                      FORWARD PASS                                    │
└───────────────────────────┼─────────────────────────────────────────┘
                            │
                            ↓ Embedding Lookup
                    ┌───────────────┐
                    │  Embeddings   │
                    │               │
                    │ [B×S×D]       │
                    └───────┬───────┘
                            │
                            ↓ For each layer
                    ┌───────────────┐
                    │   Attention   │
                    │               │
                    │ Q = X × Wq    │
                    │ K = X × Wk    │
                    │ V = X × Wv    │
                    │               │
                    │ Attn = softmax│
                    │   (QK'/√d) V  │
                    └───────┬───────┘
                            │
                            ↓ Add & Norm
                    ┌───────────────┐
                    │  LayerNorm    │
                    │               │
                    │ X = norm(X +  │
                    │     Attn)     │
                    └───────┬───────┘
                            │
                            ↓ Feed-Forward
                    ┌───────────────┐
                    │      FFN      │
                    │               │
                    │ H = ReLU(XW₁) │
                    │ Y = HW₂       │
                    └───────┬───────┘
                            │
                            ↓ Add & Norm
                    ┌───────────────┐
                    │  LayerNorm    │
                    │               │
                    │ X = norm(X+Y) │
                    └───────┬───────┘
                            │
                            ↓ Repeat for all layers
                    ┌───────────────┐
                    │ Final Hidden  │
                    │               │
                    │ [B×S×D]       │
                    └───────┬───────┘
                            │
                            ↓ Output Projection
                    ┌───────────────┐
                    │    Logits     │
                    │               │
                    │ [B×S×V]       │
                    └───────┬───────┘
                            │
┌───────────────────────────┼─────────────────────────────────────────┐
│                      LOSS CALCULATION                                │
└───────────────────────────┼─────────────────────────────────────────┘
                            │
                            ↓ Cross-Entropy
                    ┌───────────────┐
                    │  Softmax      │
                    │               │
                    │ p = exp(z) /  │
                    │     Σexp(z)   │
                    └───────┬───────┘
                            │
                            ↓
                    ┌───────────────┐
                    │     Loss      │
                    │               │
                    │ L = -log(p_t) │
                    └───────┬───────┘
                            │
┌───────────────────────────┼─────────────────────────────────────────┐
│                      BACKWARD PASS                                   │
└───────────────────────────┼─────────────────────────────────────────┘
                            │
                            ↓ ∂L/∂logits
                    ┌───────────────┐
                    │ Grad Logits   │
                    │               │
                    │ ∂L/∂z = p - y │
                    └───────┬───────┘
                            │
                            ↓ Backward through layers
                    ┌───────────────┐
                    │ Grad Hidden   │
                    │               │
                    │ ∂L/∂h         │
                    └───────┬───────┘
                            │
                            ↓ For each layer (reverse)
                    ┌───────────────┐
                    │  Grad FFN     │
                    │               │
                    │ ∂L/∂W₁, ∂L/∂W₂│
                    └───────┬───────┘
                            │
                            ↓
                    ┌───────────────┐
                    │ Grad LayerNorm│
                    │               │
                    │ ∂L/∂γ, ∂L/∂β  │
                    └───────┬───────┘
                            │
                            ↓
                    ┌───────────────┐
                    │ Grad Attention│
                    │               │
                    │ ∂L/∂Wq,Wk,Wv  │
                    └───────┬───────┘
                            │
                            ↓
                    ┌───────────────┐
                    │Grad Embeddings│
                    │               │
                    │ ∂L/∂E         │
                    └───────┬───────┘
                            │
┌───────────────────────────┼─────────────────────────────────────────┐
│                      OPTIMIZER STEP                                  │
└───────────────────────────┼─────────────────────────────────────────┘
                            │
                            ↓ Gradient Accumulation
                    ┌───────────────┐
                    │  Accumulate   │
                    │               │
                    │ g_acc += g    │
                    └───────┬───────┘
                            │
                            ↓ If accumulated enough
                    ┌───────────────┐
                    │  Scale Grads  │
                    │               │
                    │ g = g / N_acc │
                    └───────┬───────┘
                            │
                            ↓ SGD Update (⚠️ Simple)
                    ┌───────────────┐
                    │ Update Params │
                    │               │
                    │ θ = θ - η·g   │
                    └───────┬───────┘
                            │
                            ↓
                    ┌───────────────┐
                    │ Clear Grads   │
                    │               │
                    │ g = 0         │
                    └───────┬───────┘
                            │
                            ↓
                    ┌───────────────┐
                    │  Next Batch   │
                    └───────────────┘
```

---

## 3. MEMORY LAYOUT

```
┌─────────────────────────────────────────────────────────────────────┐
│                      CLLMTraining Memory                             │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  ┌────────────────────────────────────────────────────────────┐    │
│  │  Training Data                                             │    │
│  │  ┌──────────────────────────────────────────────────────┐ │    │
│  │  │ tokens: [t₁, t₂, t₃, ..., tₙ]                       │ │    │
│  │  │ Size: num_tokens × sizeof(uint32_t)                 │ │    │
│  │  └──────────────────────────────────────────────────────┘ │    │
│  └────────────────────────────────────────────────────────────┘    │
│                                                                      │
│  ┌────────────────────────────────────────────────────────────┐    │
│  │  Gradient Buffers                                          │    │
│  │  ┌──────────────────────────────────────────────────────┐ │    │
│  │  │ gradients: [∂L/∂e₁, ∂L/∂e₂, ..., ∂L/∂eₙ]           │ │    │
│  │  │ Size: vocab_size × embed_dim × sizeof(float)        │ │    │
│  │  └──────────────────────────────────────────────────────┘ │    │
│  │  ┌──────────────────────────────────────────────────────┐ │    │
│  │  │ attention_grads[layer]:                              │ │    │
│  │  │   - query_lattice: [∂L/∂Wq]                         │ │    │
│  │  │   - key_lattice: [∂L/∂Wk]                           │ │    │
│  │  │   - value_lattice: [∂L/∂Wv]                         │ │    │
│  │  │ Size per layer: 3 × D² × sizeof(float)              │ │    │
│  │  └──────────────────────────────────────────────────────┘ │    │
│  │  ┌──────────────────────────────────────────────────────┐ │    │
│  │  │ ff_grads[layer]:                                     │ │    │
│  │  │   - w1_lattice: [∂L/∂W₁]                            │ │    │
│  │  │   - w2_lattice: [∂L/∂W₂]                            │ │    │
│  │  │   - bias1, bias2                                     │ │    │
│  │  │ Size per layer: (D×H + H×D) × sizeof(float)         │ │    │
│  │  └──────────────────────────────────────────────────────┘ │    │
│  └────────────────────────────────────────────────────────────┘    │
│                                                                      │
│  ┌────────────────────────────────────────────────────────────┐    │
│  │  Forward Pass Storage                                      │    │
│  │  ┌──────────────────────────────────────────────────────┐ │    │
│  │  │ input_embeddings: [B×S×D]                            │ │    │
│  │  │ layer_inputs[L]: [B×S×D] per layer                   │ │    │
│  │  │ attention_outputs[L]: [B×S×D] per layer              │ │    │
│  │  │ ff_hidden[L]: [B×S×4D] per layer                     │ │    │
│  │  │ ff_outputs[L]: [B×S×D] per layer                     │ │    │
│  │  │ layer_outputs[L]: [B×S×D] per layer                  │ │    │
│  │  │ final_hidden: [B×S×D]                                │ │    │
│  │  │ logits: [B×S×V]                                      │ │    │
│  │  └──────────────────────────────────────────────────────┘ │    │
│  └────────────────────────────────────────────────────────────┘    │
│                                                                      │
│  ┌────────────────────────────────────────────────────────────┐    │
│  │  Backward Pass Buffers (Pre-allocated)                    │    │
│  │  ┌──────────────────────────────────────────────────────┐ │    │
│  │  │ backward_embeddings: [B×S×D]                         │ │    │
│  │  │ backward_grad_output: [B×S×D]                        │ │    │
│  │  │ backward_layer_input: [D]                            │ │    │
│  │  │ backward_layer_grad: [D]                             │ │    │
│  │  │ backward_temp_grad: [D]                              │ │    │
│  │  └──────────────────────────────────────────────────────┘ │    │
│  └────────────────────────────────────────────────────────────┘    │
│                                                                      │
│  ┌────────────────────────────────────────────────────────────┐    │
│  │  Embedding Cache (Optimization)                            │    │
│  │  ┌──────────────────────────────────────────────────────┐ │    │
│  │  │ cached_input_embeddings: [B×S×D]                     │ │    │
│  │  │ cached_target_embeddings: [B×S×D]                    │ │    │
│  │  └──────────────────────────────────────────────────────┘ │    │
│  └────────────────────────────────────────────────────────────┘    │
│                                                                      │
│  ┌────────────────────────────────────────────────────────────┐    │
│  │  Mixed Precision (Optional)                                │    │
│  │  ┌──────────────────────────────────────────────────────┐ │    │
│  │  │ master_weights: [total_params] (FP32)                │ │    │
│  │  │ fp16_activations: [activation_size] (FP16)           │ │    │
│  │  │ fp16_gradients: [gradient_size] (FP16)               │ │    │
│  │  └──────────────────────────────────────────────────────┘ │    │
│  └────────────────────────────────────────────────────────────┘    │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘

Legend:
  B = Batch Size
  S = Sequence Length
  D = Embedding Dimension
  V = Vocabulary Size
  L = Number of Layers
  H = Hidden Dimension (typically 4×D)
```

---

## 4. CRYSTALLINE LATTICE ABACUS LIFECYCLE

```
┌─────────────────────────────────────────────────────────────────────┐
│                    APPLICATION LIFECYCLE                             │
└─────────────────────────────────────────────────────────────────────┘

    Program Start
         │
         ↓
    ┌─────────────────┐
    │ abacus_create() │
    └────────┬────────┘
             │
             ↓
    ┌─────────────────────────────────────────────────────────────┐
    │              CrystalAbacus Instance                          │
    │                                                              │
    │  ┌────────────────────────────────────────────────────────┐ │
    │  │  State:                                                │ │
    │  │    primes: []                                          │ │
    │  │    num_primes: 0                                       │ │
    │  │    capacity: 0                                         │ │
    │  │    candidate: 2                                        │ │
    │  │    seen: NULL                                          │ │
    │  └────────────────────────────────────────────────────────┘ │
    └──────────────────────────┬───────────────────────────────────┘
                               │
                               │ Throughout Application Lifetime
                               │
         ┌─────────────────────┼─────────────────────┐
         │                     │                     │
         ↓                     ↓                     ↓
    ┌─────────┐          ┌─────────┐          ┌─────────┐
    │ Usage 1 │          │ Usage 2 │          │ Usage N │
    └────┬────┘          └────┬────┘          └────┬────┘
         │                    │                    │
         ↓                    ↓                    ↓
    abacus_next_prime()  abacus_is_prime()   abacus_next_prime()
         │                    │                    │
         ↓                    ↓                    ↓
    ┌─────────────────────────────────────────────────────────────┐
    │              CrystalAbacus Instance (Updated)                │
    │                                                              │
    │  ┌────────────────────────────────────────────────────────┐ │
    │  │  State:                                                │ │
    │  │    primes: [2, 3, 5, 7, 11, 13, ...]                  │ │
    │  │    num_primes: N                                       │ │
    │  │    capacity: M (M >= N)                                │ │
    │  │    candidate: next_candidate                           │ │
    │  │    seen: {hash_table}                                  │ │
    │  └────────────────────────────────────────────────────────┘ │
    │                                                              │
    │  Key Properties:                                             │
    │    ✓ Primes cached (no recalculation)                       │
    │    ✓ State persists across calls                            │
    │    ✓ Efficient lookup via hash table                        │
    │    ✓ Dynamic growth as needed                               │
    └──────────────────────────┬───────────────────────────────────┘
                               │
                               │ At Application Exit
                               ↓
                        ┌─────────────┐
                        │ abacus_free()│
                        └──────┬──────┘
                               │
                               ↓
                        Memory Released
                               │
                               ↓
                          Program End
```

---

## 5. OPTIMIZER COMPARISON

### Current Implementation (SGD)

```
┌─────────────────────────────────────────────────────────────────────┐
│                    Simple SGD Update                                 │
└─────────────────────────────────────────────────────────────────────┘

    Gradient Computation
            │
            ↓
    ┌───────────────┐
    │  Accumulate   │
    │               │
    │ g_acc += g    │
    └───────┬───────┘
            │
            ↓ If accumulated N steps
    ┌───────────────┐
    │  Scale        │
    │               │
    │ g = g_acc / N │
    └───────┬───────┘
            │
            ↓
    ┌───────────────┐
    │  Update       │
    │               │
    │ θ = θ - η·g   │
    └───────┬───────┘
            │
            ↓
    ┌───────────────┐
    │  Clear        │
    │               │
    │ g_acc = 0     │
    └───────────────┘

Characteristics:
  • Simple and fast
  • No momentum
  • Fixed learning rate
  • Can be unstable
  • Slower convergence
```

### Recommended Implementation (Adam)

```
┌─────────────────────────────────────────────────────────────────────┐
│                    Adam Optimizer Update                             │
└─────────────────────────────────────────────────────────────────────┘

    Gradient Computation
            │
            ↓
    ┌───────────────┐
    │  Accumulate   │
    │               │
    │ g_acc += g    │
    └───────┬───────┘
            │
            ↓ If accumulated N steps
    ┌───────────────┐
    │  Scale        │
    │               │
    │ g = g_acc / N │
    └───────┬───────┘
            │
            ↓
    ┌───────────────┐
    │ Update First  │
    │   Moment      │
    │               │
    │ m = β₁·m +    │
    │   (1-β₁)·g    │
    └───────┬───────┘
            │
            ↓
    ┌───────────────┐
    │ Update Second │
    │   Moment      │
    │               │
    │ v = β₂·v +    │
    │   (1-β₂)·g²   │
    └───────┬───────┘
            │
            ↓
    ┌───────────────┐
    │ Bias Correct  │
    │               │
    │ m̂ = m/(1-β₁ᵗ) │
    │ v̂ = v/(1-β₂ᵗ) │
    └───────┬───────┘
            │
            ↓
    ┌───────────────┐
    │  Update       │
    │               │
    │ θ = θ - η·m̂  │
    │     /(√v̂ + ε) │
    └───────┬───────┘
            │
            ↓
    ┌───────────────┐
    │  Clear        │
    │               │
    │ g_acc = 0     │
    └───────────────┘

Characteristics:
  • Adaptive learning rates
  • Momentum (m)
  • Second moment (v)
  • Bias correction
  • Faster convergence
  • More stable
```

---

**End of Architecture Diagrams**